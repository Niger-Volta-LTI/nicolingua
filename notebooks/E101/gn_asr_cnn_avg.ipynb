{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import h5py\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "pprint = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_LANGUAGES = {'_language_independent', 'francais', 'maninka', 'pular', 'susu'}\n",
    "\n",
    "BASE_DIR = Path('/media/xtrem/data/experiments/nicolingua-0002-va-asr/datasets/gn_va_asr_dataset_2020-08-24_02')\n",
    "\n",
    "\n",
    "ANNOTATIONS_PATH = BASE_DIR/ \"annotated_segments\" / \"metadata.csv\"\n",
    "\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"wav2vec_features-c\", \n",
    "    \"wav2vec_features-z\", \n",
    "    \"retrained-wav2vec_features-c\", \n",
    "    \"retrained-wav2vec_features-z\"\n",
    "]\n",
    "\n",
    "# CONV_POOLING_TYPES = ['avg', 'max']\n",
    "CONV_POOLING_TYPES = ['avg']\n",
    "OBJECTIVE_TYPES = ['voice_cmd', 'voice_cmd__and__voice_cmd_lng']\n",
    "CONV_DROPOUT_PROBABILITIES = [0.3]\n",
    "FC_DROPOUT_PROBABILITIES = [0.3]\n",
    "\n",
    "\n",
    "TRAIN_PERCENT = .7\n",
    "FOLD_COUNT = 5\n",
    "\n",
    "\n",
    "RESULTS_DIR = f'results_101'\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 512\n",
    "MAX_FEATURE_SEQUENCE_LENGTH = 200\n",
    "\n",
    "GPU_ID = 0\n",
    "device = torch.device(f\"cuda:{GPU_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & shuffle metadata records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata():\n",
    "    with open(ANNOTATIONS_PATH) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            if r['language'] in SELECTED_LANGUAGES:\n",
    "                yield r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "metadata_records = list(load_metadata())\n",
    "\n",
    "# shuffle\n",
    "metadata_shuffler_rs = np.random.RandomState(seed=42)\n",
    "metadata_shuffler_rs.shuffle(metadata_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "device_id: \n",
      "\td001,d002,d003\n",
      "\n",
      "language: \n",
      "\t_language_independent,francais,maninka,pular,susu\n",
      "\n",
      "speaker_gender: \n",
      "\tF,M\n",
      "\n",
      "speaker_mothertongue: \n",
      "\tmaninka,pular,susu\n"
     ]
    }
   ],
   "source": [
    "bias_category_fields = [\n",
    "     \"device_id\"\n",
    "    ,\"language\"\n",
    "    ,\"speaker_gender\"\n",
    "    ,\"speaker_mothertongue\"\n",
    "]\n",
    "\n",
    "bias_categories = {}\n",
    "for c in bias_category_fields:\n",
    "    bias_categories[c] = sorted({r[c] for r in metadata_records})\n",
    "\n",
    "\n",
    "_ = [print(f\"\\n{k}: \\n\\t{','.join(v)}\") for k,v in bias_categories.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes - Voice Commands\n",
      "   0: 101_wake_word__francais\n",
      "   1: 101_wake_word__maninka\n",
      "   2: 101_wake_word__pular\n",
      "   3: 101_wake_word__susu\n",
      "   4: 201_add_contact__francais\n",
      "   5: 201_add_contact__maninka\n",
      "   6: 201_add_contact__pular\n",
      "   7: 201_add_contact__susu\n",
      "   8: 202_search_contact__francais\n",
      "   9: 202_search_contact__maninka\n",
      "  10: 202_search_contact__pular\n",
      "  11: 202_search_contact__susu\n",
      "  12: 203_update_contact__francais\n",
      "  13: 203_update_contact__maninka\n",
      "  14: 203_update_contact__pular\n",
      "  15: 203_update_contact__susu\n",
      "  16: 204_delete_contact__francais\n",
      "  17: 204_delete_contact__maninka\n",
      "  18: 204_delete_contact__pular\n",
      "  19: 204_delete_contact__susu\n",
      "  20: 205_call_contact__francais\n",
      "  21: 205_call_contact__maninka\n",
      "  22: 205_call_contact__pular\n",
      "  23: 205_call_contact__susu\n",
      "  24: 206_yes__francais\n",
      "  25: 206_yes__maninka\n",
      "  26: 206_yes__pular\n",
      "  27: 206_yes__susu\n",
      "  28: 207_no__francais\n",
      "  29: 207_no__maninka\n",
      "  30: 207_no__pular\n",
      "  31: 207_no__susu\n",
      "  32: 301_zero__francais\n",
      "  33: 301_zero__maninka\n",
      "  34: 301_zero__pular\n",
      "  35: 301_zero__susu\n",
      "  36: 302_one__francais\n",
      "  37: 302_one__maninka\n",
      "  38: 302_one__pular\n",
      "  39: 302_one__susu\n",
      "  40: 303_two__francais\n",
      "  41: 303_two__maninka\n",
      "  42: 303_two__pular\n",
      "  43: 303_two__susu\n",
      "  44: 304_three__francais\n",
      "  45: 304_three__maninka\n",
      "  46: 304_three__pular\n",
      "  47: 304_three__susu\n",
      "  48: 305_four__francais\n",
      "  49: 305_four__maninka\n",
      "  50: 305_four__pular\n",
      "  51: 305_four__susu\n",
      "  52: 306_five__francais\n",
      "  53: 306_five__maninka\n",
      "  54: 306_five__pular\n",
      "  55: 306_five__susu\n",
      "  56: 307_six__francais\n",
      "  57: 307_six__maninka\n",
      "  58: 307_six__pular\n",
      "  59: 307_six__susu\n",
      "  60: 308_seven__francais\n",
      "  61: 308_seven__maninka\n",
      "  62: 308_seven__pular\n",
      "  63: 308_seven__susu\n",
      "  64: 309_eight__francais\n",
      "  65: 309_eight__maninka\n",
      "  66: 309_eight__pular\n",
      "  67: 309_eight__susu\n",
      "  68: 310_nine__francais\n",
      "  69: 310_nine__maninka\n",
      "  70: 310_nine__pular\n",
      "  71: 310_nine__susu\n",
      "  72: 401_mom__francais\n",
      "  73: 401_mom__maninka\n",
      "  74: 401_mom__pular\n",
      "  75: 401_mom__susu\n",
      "  76: 402_dad__francais\n",
      "  77: 402_dad__maninka\n",
      "  78: 402_dad__pular\n",
      "  79: 402_dad__susu\n",
      "  80: 501_fatoumata___language_independent\n",
      "  81: 502_mamadou___language_independent\n",
      "  82: 503_mariama___language_independent\n",
      "  83: 504_mohamed___language_independent\n",
      "  84: 505_kadiatou___language_independent\n",
      "  85: 506_ibrahima___language_independent\n",
      "  86: 507_aissatou___language_independent\n",
      "  87: 508_aminata___language_independent\n",
      "  88: 509_alpha___language_independent\n",
      "  89: 510_thierno___language_independent\n",
      "  90: 511_abdoulaye___language_independent\n",
      "  91: 512_aboubacar___language_independent\n",
      "  92: 513_amadou___language_independent\n",
      "  93: 514_fanta___language_independent\n",
      "  94: 515_mariame___language_independent\n",
      "  95: 516_oumou___language_independent\n",
      "  96: 517_ousmane___language_independent\n",
      "  97: 518_adama___language_independent\n",
      "  98: 519_marie___language_independent\n",
      "  99: 520_moussa___language_independent\n",
      " 100: 521_aissata___language_independent\n",
      " 101: 522_hawa___language_independent\n",
      " 102: 523_sekou___language_independent\n",
      " 103: 524_hadja___language_independent\n",
      " 104: 525_djenabou___language_independent\n",
      "---------------------\n",
      "Classes - Voice Command Languages\n",
      "  0: _language_independent\n",
      "  1: francais\n",
      "  2: maninka\n",
      "  3: pular\n",
      "  4: susu\n",
      "---------------------\n",
      "Classes - Speaker Mothertongues\n",
      "  0: maninka\n",
      "  1: pular\n",
      "  2: susu\n",
      "---------------------\n",
      "Classes - Speaker Gender\n",
      "  0: F\n",
      "  1: M\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# VOICE COMMANDS\n",
    "voice_cmd_class_names = sorted({r['label'] for r in metadata_records})\n",
    "voice_cmd_class_count = len(voice_cmd_class_names)\n",
    "voice_cmd_class_id_by_name = {c:i for i, c in enumerate(voice_cmd_class_names)}\n",
    "\n",
    "print(\"Classes - Voice Commands\")\n",
    "_ = [print(f\"{v:4}: {k}\") for k,v in voice_cmd_class_id_by_name.items()]\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "\n",
    "# VOICE COMMAND LANGUAGES\n",
    "voice_cmd_lng_class_names = sorted({r['language'] for r in metadata_records})\n",
    "voice_cmd_lng_class_count = len(voice_cmd_lng_class_names)\n",
    "voice_cmd_lng_class_id_by_name = {c:i for i, c in enumerate(voice_cmd_lng_class_names)}\n",
    "\n",
    "print(\"Classes - Voice Command Languages\")\n",
    "_ = [print(f\"{v:3}: {k}\") for k,v in voice_cmd_lng_class_id_by_name.items()]\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# SPEAKER MOTHERTONGUE\n",
    "spkr_mothertongue_class_names = sorted({r['speaker_mothertongue'] for r in metadata_records})\n",
    "spkr_mothertongue_class_count = len(spkr_mothertongue_class_names)\n",
    "spkr_mothertongue_class_id_by_name = {c:i for i,c in enumerate(spkr_mothertongue_class_names)}\n",
    "\n",
    "print(\"Classes - Speaker Mothertongues\")\n",
    "_ = [print(f\"{v:3}: {k}\") for k,v in spkr_mothertongue_class_id_by_name.items()]\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# SPEAKER GENDER\n",
    "spkr_gender_class_names = sorted({r['speaker_gender'] for r in metadata_records})\n",
    "spkr_gender_class_count = len(spkr_gender_class_names)\n",
    "spkr_gender_class_id_by_name = {c:i for i, c in enumerate(spkr_gender_class_names)}\n",
    "\n",
    "print(\"Classes - Speaker Gender\")\n",
    "_ = [print(f\"{v:3}: {k}\") for k,v in spkr_gender_class_id_by_name.items()]\n",
    "\n",
    "print(\"----------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count_by_attribute(records, attribute_names):\n",
    "    attribute_name_instances = {}\n",
    "    for attribute_name in attribute_names:\n",
    "        attribute_name_instances[attribute_name] = {r[attribute_name] for r in records}\n",
    "        \n",
    "    l = [attribute_name_instances[attribute_name] for attribute_name in attribute_names]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for attribute_values in sorted(itertools.product(*l)):\n",
    "        \n",
    "        def record_match(r):\n",
    "            for i in range(len(attribute_names)):\n",
    "                if r[attribute_names[i]] != attribute_values[i]:\n",
    "                    return False\n",
    "            return True\n",
    "            \n",
    "        record_instances = [r for r in records if record_match(r)]\n",
    "        count = len(record_instances)\n",
    "        \n",
    "        yield (attribute_values, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECORDS BY DEVICE\n",
      "\t(('d001',), 2759)\n",
      "\t(('d002',), 2741)\n",
      "\t(('d003',), 2759)\n",
      "\n",
      "RECORDS BY LANGUAGE\n",
      "\t(('_language_independent',), 3072)\n",
      "\t(('francais',), 1260)\n",
      "\t(('maninka',), 1356)\n",
      "\t(('pular',), 909)\n",
      "\t(('susu',), 1662)\n",
      "\n",
      "RECORDS BY GENDER\n",
      "\t(('F',), 2820)\n",
      "\t(('M',), 5439)\n",
      "\n",
      "RECORDS BY AGE\n",
      "\t(('12',), 237)\n",
      "\t(('13',), 252)\n",
      "\t(('15',), 603)\n",
      "\t(('17',), 1050)\n",
      "\t(('18',), 855)\n",
      "\t(('19',), 273)\n",
      "\t(('20',), 291)\n",
      "\t(('27',), 255)\n",
      "\t(('28',), 183)\n",
      "\t(('29',), 237)\n",
      "\t(('31',), 255)\n",
      "\t(('32',), 291)\n",
      "\t(('33',), 183)\n",
      "\t(('34',), 129)\n",
      "\t(('35',), 498)\n",
      "\t(('37',), 309)\n",
      "\t(('38',), 441)\n",
      "\t(('43',), 183)\n",
      "\t(('44',), 540)\n",
      "\t(('5',), 129)\n",
      "\t(('55',), 183)\n",
      "\t(('61',), 390)\n",
      "\t(('63',), 237)\n",
      "\t(('67',), 255)\n",
      "\n",
      "RECORDS BY SPEAKER\n",
      "\t(('s001',), 183)\n",
      "\t(('s002',), 129)\n",
      "\t(('s003',), 183)\n",
      "\t(('s004',), 183)\n",
      "\t(('s005',), 129)\n",
      "\t(('s006',), 129)\n",
      "\t(('s007',), 183)\n",
      "\t(('s008',), 129)\n",
      "\t(('s009',), 237)\n",
      "\t(('s010',), 291)\n",
      "\t(('s011',), 129)\n",
      "\t(('s012',), 183)\n",
      "\t(('s013',), 129)\n",
      "\t(('s014',), 237)\n",
      "\t(('s015',), 291)\n",
      "\t(('s016',), 183)\n",
      "\t(('s017',), 237)\n",
      "\t(('s018',), 237)\n",
      "\t(('s019',), 237)\n",
      "\t(('s020',), 291)\n",
      "\t(('s021',), 273)\n",
      "\t(('s022',), 183)\n",
      "\t(('s023',), 309)\n",
      "\t(('s024',), 255)\n",
      "\t(('s025',), 315)\n",
      "\t(('s026',), 252)\n",
      "\t(('s027',), 540)\n",
      "\t(('s028',), 540)\n",
      "\t(('s029',), 390)\n",
      "\t(('s030',), 195)\n",
      "\t(('s031',), 252)\n",
      "\t(('s032',), 315)\n",
      "\t(('s033',), 255)\n",
      "\t(('s034',), 255)\n",
      "\n",
      "RECORDS BY SPEAKER BY LANGUAGE\n",
      "\t(('s001', '_language_independent'), 75)\n",
      "\t(('s001', 'francais'), 0)\n",
      "\t(('s001', 'maninka'), 54)\n",
      "\t(('s001', 'pular'), 0)\n",
      "\t(('s001', 'susu'), 54)\n",
      "\t(('s002', '_language_independent'), 75)\n",
      "\t(('s002', 'francais'), 0)\n",
      "\t(('s002', 'maninka'), 54)\n",
      "\t(('s002', 'pular'), 0)\n",
      "\t(('s002', 'susu'), 0)\n",
      "\t(('s003', '_language_independent'), 75)\n",
      "\t(('s003', 'francais'), 0)\n",
      "\t(('s003', 'maninka'), 54)\n",
      "\t(('s003', 'pular'), 0)\n",
      "\t(('s003', 'susu'), 54)\n",
      "\t(('s004', '_language_independent'), 75)\n",
      "\t(('s004', 'francais'), 0)\n",
      "\t(('s004', 'maninka'), 54)\n",
      "\t(('s004', 'pular'), 0)\n",
      "\t(('s004', 'susu'), 54)\n",
      "\t(('s005', '_language_independent'), 75)\n",
      "\t(('s005', 'francais'), 0)\n",
      "\t(('s005', 'maninka'), 0)\n",
      "\t(('s005', 'pular'), 0)\n",
      "\t(('s005', 'susu'), 54)\n",
      "\t(('s006', '_language_independent'), 75)\n",
      "\t(('s006', 'francais'), 0)\n",
      "\t(('s006', 'maninka'), 54)\n",
      "\t(('s006', 'pular'), 0)\n",
      "\t(('s006', 'susu'), 0)\n",
      "\t(('s007', '_language_independent'), 75)\n",
      "\t(('s007', 'francais'), 0)\n",
      "\t(('s007', 'maninka'), 0)\n",
      "\t(('s007', 'pular'), 54)\n",
      "\t(('s007', 'susu'), 54)\n",
      "\t(('s008', '_language_independent'), 75)\n",
      "\t(('s008', 'francais'), 0)\n",
      "\t(('s008', 'maninka'), 54)\n",
      "\t(('s008', 'pular'), 0)\n",
      "\t(('s008', 'susu'), 0)\n",
      "\t(('s009', '_language_independent'), 75)\n",
      "\t(('s009', 'francais'), 54)\n",
      "\t(('s009', 'maninka'), 0)\n",
      "\t(('s009', 'pular'), 54)\n",
      "\t(('s009', 'susu'), 54)\n",
      "\t(('s010', '_language_independent'), 75)\n",
      "\t(('s010', 'francais'), 54)\n",
      "\t(('s010', 'maninka'), 54)\n",
      "\t(('s010', 'pular'), 54)\n",
      "\t(('s010', 'susu'), 54)\n",
      "\t(('s011', '_language_independent'), 75)\n",
      "\t(('s011', 'francais'), 0)\n",
      "\t(('s011', 'maninka'), 0)\n",
      "\t(('s011', 'pular'), 0)\n",
      "\t(('s011', 'susu'), 54)\n",
      "\t(('s012', '_language_independent'), 75)\n",
      "\t(('s012', 'francais'), 54)\n",
      "\t(('s012', 'maninka'), 0)\n",
      "\t(('s012', 'pular'), 0)\n",
      "\t(('s012', 'susu'), 54)\n",
      "\t(('s013', '_language_independent'), 75)\n",
      "\t(('s013', 'francais'), 0)\n",
      "\t(('s013', 'maninka'), 0)\n",
      "\t(('s013', 'pular'), 0)\n",
      "\t(('s013', 'susu'), 54)\n",
      "\t(('s014', '_language_independent'), 75)\n",
      "\t(('s014', 'francais'), 54)\n",
      "\t(('s014', 'maninka'), 54)\n",
      "\t(('s014', 'pular'), 0)\n",
      "\t(('s014', 'susu'), 54)\n",
      "\t(('s015', '_language_independent'), 75)\n",
      "\t(('s015', 'francais'), 54)\n",
      "\t(('s015', 'maninka'), 54)\n",
      "\t(('s015', 'pular'), 54)\n",
      "\t(('s015', 'susu'), 54)\n",
      "\t(('s016', '_language_independent'), 75)\n",
      "\t(('s016', 'francais'), 0)\n",
      "\t(('s016', 'maninka'), 54)\n",
      "\t(('s016', 'pular'), 0)\n",
      "\t(('s016', 'susu'), 54)\n",
      "\t(('s017', '_language_independent'), 75)\n",
      "\t(('s017', 'francais'), 54)\n",
      "\t(('s017', 'maninka'), 0)\n",
      "\t(('s017', 'pular'), 54)\n",
      "\t(('s017', 'susu'), 54)\n",
      "\t(('s018', '_language_independent'), 75)\n",
      "\t(('s018', 'francais'), 54)\n",
      "\t(('s018', 'maninka'), 0)\n",
      "\t(('s018', 'pular'), 54)\n",
      "\t(('s018', 'susu'), 54)\n",
      "\t(('s019', '_language_independent'), 75)\n",
      "\t(('s019', 'francais'), 54)\n",
      "\t(('s019', 'maninka'), 54)\n",
      "\t(('s019', 'pular'), 0)\n",
      "\t(('s019', 'susu'), 54)\n",
      "\t(('s020', '_language_independent'), 75)\n",
      "\t(('s020', 'francais'), 54)\n",
      "\t(('s020', 'maninka'), 54)\n",
      "\t(('s020', 'pular'), 54)\n",
      "\t(('s020', 'susu'), 54)\n",
      "\t(('s021', '_language_independent'), 75)\n",
      "\t(('s021', 'francais'), 54)\n",
      "\t(('s021', 'maninka'), 54)\n",
      "\t(('s021', 'pular'), 54)\n",
      "\t(('s021', 'susu'), 36)\n",
      "\t(('s022', '_language_independent'), 75)\n",
      "\t(('s022', 'francais'), 0)\n",
      "\t(('s022', 'maninka'), 54)\n",
      "\t(('s022', 'pular'), 0)\n",
      "\t(('s022', 'susu'), 54)\n",
      "\t(('s023', '_language_independent'), 75)\n",
      "\t(('s023', 'francais'), 60)\n",
      "\t(('s023', 'maninka'), 60)\n",
      "\t(('s023', 'pular'), 60)\n",
      "\t(('s023', 'susu'), 54)\n",
      "\t(('s024', '_language_independent'), 75)\n",
      "\t(('s024', 'francais'), 60)\n",
      "\t(('s024', 'maninka'), 60)\n",
      "\t(('s024', 'pular'), 0)\n",
      "\t(('s024', 'susu'), 60)\n",
      "\t(('s025', '_language_independent'), 75)\n",
      "\t(('s025', 'francais'), 60)\n",
      "\t(('s025', 'maninka'), 60)\n",
      "\t(('s025', 'pular'), 60)\n",
      "\t(('s025', 'susu'), 60)\n",
      "\t(('s026', '_language_independent'), 72)\n",
      "\t(('s026', 'francais'), 60)\n",
      "\t(('s026', 'maninka'), 60)\n",
      "\t(('s026', 'pular'), 0)\n",
      "\t(('s026', 'susu'), 60)\n",
      "\t(('s027', '_language_independent'), 300)\n",
      "\t(('s027', 'francais'), 60)\n",
      "\t(('s027', 'maninka'), 60)\n",
      "\t(('s027', 'pular'), 60)\n",
      "\t(('s027', 'susu'), 60)\n",
      "\t(('s028', '_language_independent'), 300)\n",
      "\t(('s028', 'francais'), 60)\n",
      "\t(('s028', 'maninka'), 60)\n",
      "\t(('s028', 'pular'), 60)\n",
      "\t(('s028', 'susu'), 60)\n",
      "\t(('s029', '_language_independent'), 150)\n",
      "\t(('s029', 'francais'), 60)\n",
      "\t(('s029', 'maninka'), 60)\n",
      "\t(('s029', 'pular'), 60)\n",
      "\t(('s029', 'susu'), 60)\n",
      "\t(('s030', '_language_independent'), 75)\n",
      "\t(('s030', 'francais'), 60)\n",
      "\t(('s030', 'maninka'), 0)\n",
      "\t(('s030', 'pular'), 60)\n",
      "\t(('s030', 'susu'), 0)\n",
      "\t(('s031', '_language_independent'), 75)\n",
      "\t(('s031', 'francais'), 60)\n",
      "\t(('s031', 'maninka'), 0)\n",
      "\t(('s031', 'pular'), 57)\n",
      "\t(('s031', 'susu'), 60)\n",
      "\t(('s032', '_language_independent'), 75)\n",
      "\t(('s032', 'francais'), 60)\n",
      "\t(('s032', 'maninka'), 60)\n",
      "\t(('s032', 'pular'), 60)\n",
      "\t(('s032', 'susu'), 60)\n",
      "\t(('s033', '_language_independent'), 75)\n",
      "\t(('s033', 'francais'), 60)\n",
      "\t(('s033', 'maninka'), 60)\n",
      "\t(('s033', 'pular'), 0)\n",
      "\t(('s033', 'susu'), 60)\n",
      "\t(('s034', '_language_independent'), 75)\n",
      "\t(('s034', 'francais'), 60)\n",
      "\t(('s034', 'maninka'), 60)\n",
      "\t(('s034', 'pular'), 0)\n",
      "\t(('s034', 'susu'), 60)\n",
      "\n",
      "RECORDS BY SPEAKER BY LABEL\n",
      "\t(('101_wake_word__francais',), 66)\n",
      "\t(('101_wake_word__maninka',), 72)\n",
      "\t(('101_wake_word__pular',), 48)\n",
      "\t(('101_wake_word__susu',), 89)\n",
      "\t(('201_add_contact__francais',), 66)\n",
      "\t(('201_add_contact__maninka',), 72)\n",
      "\t(('201_add_contact__pular',), 48)\n",
      "\t(('201_add_contact__susu',), 89)\n",
      "\t(('202_search_contact__francais',), 66)\n",
      "\t(('202_search_contact__maninka',), 72)\n",
      "\t(('202_search_contact__pular',), 48)\n",
      "\t(('202_search_contact__susu',), 89)\n",
      "\t(('203_update_contact__francais',), 66)\n",
      "\t(('203_update_contact__maninka',), 72)\n",
      "\t(('203_update_contact__pular',), 48)\n",
      "\t(('203_update_contact__susu',), 89)\n",
      "\t(('204_delete_contact__francais',), 66)\n",
      "\t(('204_delete_contact__maninka',), 72)\n",
      "\t(('204_delete_contact__pular',), 48)\n",
      "\t(('204_delete_contact__susu',), 89)\n",
      "\t(('205_call_contact__francais',), 66)\n",
      "\t(('205_call_contact__maninka',), 72)\n",
      "\t(('205_call_contact__pular',), 45)\n",
      "\t(('205_call_contact__susu',), 89)\n",
      "\t(('206_yes__francais',), 66)\n",
      "\t(('206_yes__maninka',), 72)\n",
      "\t(('206_yes__pular',), 48)\n",
      "\t(('206_yes__susu',), 89)\n",
      "\t(('207_no__francais',), 66)\n",
      "\t(('207_no__maninka',), 72)\n",
      "\t(('207_no__pular',), 48)\n",
      "\t(('207_no__susu',), 89)\n",
      "\t(('301_zero__francais',), 66)\n",
      "\t(('301_zero__maninka',), 72)\n",
      "\t(('301_zero__pular',), 48)\n",
      "\t(('301_zero__susu',), 89)\n",
      "\t(('302_one__francais',), 66)\n",
      "\t(('302_one__maninka',), 72)\n",
      "\t(('302_one__pular',), 48)\n",
      "\t(('302_one__susu',), 89)\n",
      "\t(('303_two__francais',), 66)\n",
      "\t(('303_two__maninka',), 72)\n",
      "\t(('303_two__pular',), 48)\n",
      "\t(('303_two__susu',), 89)\n",
      "\t(('304_three__francais',), 66)\n",
      "\t(('304_three__maninka',), 72)\n",
      "\t(('304_three__pular',), 48)\n",
      "\t(('304_three__susu',), 89)\n",
      "\t(('305_four__francais',), 66)\n",
      "\t(('305_four__maninka',), 72)\n",
      "\t(('305_four__pular',), 48)\n",
      "\t(('305_four__susu',), 89)\n",
      "\t(('306_five__francais',), 66)\n",
      "\t(('306_five__maninka',), 72)\n",
      "\t(('306_five__pular',), 48)\n",
      "\t(('306_five__susu',), 89)\n",
      "\t(('307_six__francais',), 66)\n",
      "\t(('307_six__maninka',), 72)\n",
      "\t(('307_six__pular',), 48)\n",
      "\t(('307_six__susu',), 89)\n",
      "\t(('308_seven__francais',), 66)\n",
      "\t(('308_seven__maninka',), 72)\n",
      "\t(('308_seven__pular',), 48)\n",
      "\t(('308_seven__susu',), 89)\n",
      "\t(('309_eight__francais',), 66)\n",
      "\t(('309_eight__maninka',), 72)\n",
      "\t(('309_eight__pular',), 48)\n",
      "\t(('309_eight__susu',), 89)\n",
      "\t(('310_nine__francais',), 66)\n",
      "\t(('310_nine__maninka',), 72)\n",
      "\t(('310_nine__pular',), 48)\n",
      "\t(('310_nine__susu',), 89)\n",
      "\t(('401_mom__francais',), 36)\n",
      "\t(('401_mom__maninka',), 30)\n",
      "\t(('401_mom__pular',), 24)\n",
      "\t(('401_mom__susu',), 30)\n",
      "\t(('402_dad__francais',), 36)\n",
      "\t(('402_dad__maninka',), 30)\n",
      "\t(('402_dad__pular',), 24)\n",
      "\t(('402_dad__susu',), 30)\n",
      "\t(('501_fatoumata___language_independent',), 123)\n",
      "\t(('502_mamadou___language_independent',), 123)\n",
      "\t(('503_mariama___language_independent',), 123)\n",
      "\t(('504_mohamed___language_independent',), 123)\n",
      "\t(('505_kadiatou___language_independent',), 123)\n",
      "\t(('506_ibrahima___language_independent',), 123)\n",
      "\t(('507_aissatou___language_independent',), 123)\n",
      "\t(('508_aminata___language_independent',), 123)\n",
      "\t(('509_alpha___language_independent',), 123)\n",
      "\t(('510_thierno___language_independent',), 120)\n",
      "\t(('511_abdoulaye___language_independent',), 123)\n",
      "\t(('512_aboubacar___language_independent',), 123)\n",
      "\t(('513_amadou___language_independent',), 123)\n",
      "\t(('514_fanta___language_independent',), 123)\n",
      "\t(('515_mariame___language_independent',), 123)\n",
      "\t(('516_oumou___language_independent',), 123)\n",
      "\t(('517_ousmane___language_independent',), 123)\n",
      "\t(('518_adama___language_independent',), 123)\n",
      "\t(('519_marie___language_independent',), 123)\n",
      "\t(('520_moussa___language_independent',), 123)\n",
      "\t(('521_aissata___language_independent',), 123)\n",
      "\t(('522_hawa___language_independent',), 123)\n",
      "\t(('523_sekou___language_independent',), 123)\n",
      "\t(('524_hadja___language_independent',), 123)\n",
      "\t(('525_djenabou___language_independent',), 123)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RECORDS BY DEVICE\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['device_id']))]\n",
    "print(\"\")\n",
    "\n",
    "print(\"RECORDS BY LANGUAGE\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['language']))]\n",
    "print(\"\")\n",
    "\n",
    "print(\"RECORDS BY GENDER\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['speaker_gender']))]\n",
    "print(\"\")\n",
    "\n",
    "print(\"RECORDS BY AGE\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['speaker_age']))]\n",
    "print(\"\")\n",
    "\n",
    "print(\"RECORDS BY SPEAKER\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['speaker_id']))]\n",
    "print(\"\")\n",
    "\n",
    "print(\"RECORDS BY SPEAKER BY LANGUAGE\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['speaker_id', 'language']))]\n",
    "print(\"\")\n",
    "\n",
    "print(\"RECORDS BY SPEAKER BY LABEL\")\n",
    "_ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(metadata_records, ['label']))]\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Cross Validation Folds\n",
    "- Partition by (speaker, language)\n",
    "- Each (speaker, language) correspond to `utterance_count * device_count`\n",
    "- For each fold, all `utterance_count * device_count` records for the same speaker in the same language are either in the TRAIN or the VALIDATION sets, but not both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_records_per_fold(all_records):\n",
    "    records_per_fold = {}\n",
    "    \n",
    "    all_speaker_languages = sorted({(r['speaker_id'], r['language']) for r in all_records})\n",
    "\n",
    "    sl_count = len(all_speaker_languages)\n",
    "    all_sl_indices = range(sl_count)\n",
    "    train_sl_count = int(np.ceil(sl_count*TRAIN_PERCENT))\n",
    "    test_sl_count = sl_count - train_sl_count\n",
    "\n",
    "    for fold_index in range(FOLD_COUNT):\n",
    "        fold_rsampler = np.random.RandomState(seed=fold_index)\n",
    "\n",
    "        train_sl_index_set = set(fold_rsampler.choice(all_sl_indices, train_sl_count, replace=False))\n",
    "        train_sl_set = {all_speaker_languages[i] for i in train_sl_index_set}\n",
    "\n",
    "        test_sl_index_set = set(all_sl_indices).difference(train_sl_index_set)\n",
    "        test_sl_set = {all_speaker_languages[i] for i in test_sl_index_set}\n",
    "\n",
    "        train_records = [r for r in all_records if (r['speaker_id'], r['language']) in train_sl_set]\n",
    "        test_records = [r for r in all_records if (r['speaker_id'], r['language']) in test_sl_set]\n",
    "        \n",
    "        \n",
    "        records_per_fold[fold_index] = {\n",
    "            \"train_records\": train_records,\n",
    "            \"test_records\": test_records\n",
    "        }\n",
    "    \n",
    "    return records_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_per_fold = generate_train_test_records_per_fold(metadata_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 -- TRAIN\n",
      "\t(('s001', '_language_independent'), 75)\n",
      "\t(('s001', 'maninka'), 54)\n",
      "\t(('s001', 'susu'), 54)\n",
      "\t(('s002', '_language_independent'), 75)\n",
      "\t(('s002', 'maninka'), 54)\n",
      "\t(('s003', '_language_independent'), 75)\n",
      "\t(('s003', 'maninka'), 54)\n",
      "\t(('s003', 'susu'), 54)\n",
      "\t(('s004', '_language_independent'), 75)\n",
      "\t(('s004', 'susu'), 54)\n",
      "\t(('s005', '_language_independent'), 75)\n",
      "\t(('s006', '_language_independent'), 75)\n",
      "\t(('s006', 'maninka'), 54)\n",
      "\t(('s007', '_language_independent'), 75)\n",
      "\t(('s007', 'pular'), 54)\n",
      "\t(('s007', 'susu'), 54)\n",
      "\t(('s008', '_language_independent'), 75)\n",
      "\t(('s008', 'maninka'), 54)\n",
      "\t(('s009', 'pular'), 54)\n",
      "\t(('s009', 'susu'), 54)\n",
      "\t(('s010', '_language_independent'), 75)\n",
      "\t(('s010', 'maninka'), 54)\n",
      "\t(('s010', 'pular'), 54)\n",
      "\t(('s010', 'susu'), 54)\n",
      "\t(('s011', '_language_independent'), 75)\n",
      "\t(('s011', 'susu'), 54)\n",
      "\t(('s012', '_language_independent'), 75)\n",
      "\t(('s012', 'francais'), 54)\n",
      "\t(('s012', 'susu'), 54)\n",
      "\t(('s013', '_language_independent'), 75)\n",
      "\t(('s013', 'susu'), 54)\n",
      "\t(('s014', 'maninka'), 54)\n",
      "\t(('s015', '_language_independent'), 75)\n",
      "\t(('s015', 'francais'), 54)\n",
      "\t(('s015', 'maninka'), 54)\n",
      "\t(('s015', 'pular'), 54)\n",
      "\t(('s016', '_language_independent'), 75)\n",
      "\t(('s017', '_language_independent'), 75)\n",
      "\t(('s017', 'francais'), 54)\n",
      "\t(('s017', 'pular'), 54)\n",
      "\t(('s017', 'susu'), 54)\n",
      "\t(('s018', '_language_independent'), 75)\n",
      "\t(('s018', 'francais'), 54)\n",
      "\t(('s018', 'pular'), 54)\n",
      "\t(('s018', 'susu'), 54)\n",
      "\t(('s019', '_language_independent'), 75)\n",
      "\t(('s019', 'francais'), 54)\n",
      "\t(('s019', 'susu'), 54)\n",
      "\t(('s020', '_language_independent'), 75)\n",
      "\t(('s020', 'francais'), 54)\n",
      "\t(('s020', 'maninka'), 54)\n",
      "\t(('s020', 'pular'), 54)\n",
      "\t(('s021', 'francais'), 54)\n",
      "\t(('s021', 'pular'), 54)\n",
      "\t(('s022', 'maninka'), 54)\n",
      "\t(('s023', '_language_independent'), 75)\n",
      "\t(('s023', 'francais'), 60)\n",
      "\t(('s023', 'maninka'), 60)\n",
      "\t(('s023', 'pular'), 60)\n",
      "\t(('s024', '_language_independent'), 75)\n",
      "\t(('s025', 'maninka'), 60)\n",
      "\t(('s025', 'pular'), 60)\n",
      "\t(('s025', 'susu'), 60)\n",
      "\t(('s026', 'maninka'), 60)\n",
      "\t(('s026', 'susu'), 60)\n",
      "\t(('s027', '_language_independent'), 300)\n",
      "\t(('s027', 'francais'), 60)\n",
      "\t(('s027', 'maninka'), 60)\n",
      "\t(('s027', 'pular'), 60)\n",
      "\t(('s027', 'susu'), 60)\n",
      "\t(('s028', '_language_independent'), 300)\n",
      "\t(('s028', 'francais'), 60)\n",
      "\t(('s028', 'maninka'), 60)\n",
      "\t(('s028', 'pular'), 60)\n",
      "\t(('s028', 'susu'), 60)\n",
      "\t(('s029', '_language_independent'), 150)\n",
      "\t(('s029', 'pular'), 60)\n",
      "\t(('s029', 'susu'), 60)\n",
      "\t(('s030', '_language_independent'), 75)\n",
      "\t(('s030', 'francais'), 60)\n",
      "\t(('s030', 'pular'), 60)\n",
      "\t(('s031', '_language_independent'), 75)\n",
      "\t(('s031', 'susu'), 60)\n",
      "\t(('s032', 'maninka'), 60)\n",
      "\t(('s032', 'pular'), 60)\n",
      "\t(('s033', 'francais'), 60)\n",
      "\t(('s033', 'maninka'), 60)\n",
      "\t(('s034', 'francais'), 60)\n",
      "\t(('s034', 'susu'), 60)\n",
      "Fold 0 -- TEST\n",
      "\t(('s004', 'maninka'), 54)\n",
      "\t(('s005', 'susu'), 54)\n",
      "\t(('s009', '_language_independent'), 75)\n",
      "\t(('s009', 'francais'), 54)\n",
      "\t(('s010', 'francais'), 54)\n",
      "\t(('s014', '_language_independent'), 75)\n",
      "\t(('s014', 'francais'), 54)\n",
      "\t(('s014', 'susu'), 54)\n",
      "\t(('s015', 'susu'), 54)\n",
      "\t(('s016', 'maninka'), 54)\n",
      "\t(('s016', 'susu'), 54)\n",
      "\t(('s019', 'maninka'), 54)\n",
      "\t(('s020', 'susu'), 54)\n",
      "\t(('s021', '_language_independent'), 75)\n",
      "\t(('s021', 'maninka'), 54)\n",
      "\t(('s021', 'susu'), 36)\n",
      "\t(('s022', '_language_independent'), 75)\n",
      "\t(('s022', 'susu'), 54)\n",
      "\t(('s023', 'susu'), 54)\n",
      "\t(('s024', 'francais'), 60)\n",
      "\t(('s024', 'maninka'), 60)\n",
      "\t(('s024', 'susu'), 60)\n",
      "\t(('s025', '_language_independent'), 75)\n",
      "\t(('s025', 'francais'), 60)\n",
      "\t(('s026', '_language_independent'), 72)\n",
      "\t(('s026', 'francais'), 60)\n",
      "\t(('s029', 'francais'), 60)\n",
      "\t(('s029', 'maninka'), 60)\n",
      "\t(('s031', 'francais'), 60)\n",
      "\t(('s031', 'pular'), 57)\n",
      "\t(('s032', '_language_independent'), 75)\n",
      "\t(('s032', 'francais'), 60)\n",
      "\t(('s032', 'susu'), 60)\n",
      "\t(('s033', '_language_independent'), 75)\n",
      "\t(('s033', 'susu'), 60)\n",
      "\t(('s034', '_language_independent'), 75)\n",
      "\t(('s034', 'maninka'), 60)\n",
      "---------------------\n",
      "Fold 1 -- TRAIN\n",
      "\t(('s001', '_language_independent'), 75)\n",
      "\t(('s001', 'susu'), 54)\n",
      "\t(('s002', '_language_independent'), 75)\n",
      "\t(('s002', 'maninka'), 54)\n",
      "\t(('s004', '_language_independent'), 75)\n",
      "\t(('s004', 'susu'), 54)\n",
      "\t(('s007', '_language_independent'), 75)\n",
      "\t(('s007', 'susu'), 54)\n",
      "\t(('s008', 'maninka'), 54)\n",
      "\t(('s009', 'francais'), 54)\n",
      "\t(('s009', 'susu'), 54)\n",
      "\t(('s010', '_language_independent'), 75)\n",
      "\t(('s010', 'maninka'), 54)\n",
      "\t(('s010', 'pular'), 54)\n",
      "\t(('s011', 'susu'), 54)\n",
      "\t(('s012', '_language_independent'), 75)\n",
      "\t(('s012', 'francais'), 54)\n",
      "\t(('s012', 'susu'), 54)\n",
      "\t(('s013', '_language_independent'), 75)\n",
      "\t(('s013', 'susu'), 54)\n",
      "\t(('s014', '_language_independent'), 75)\n",
      "\t(('s014', 'maninka'), 54)\n",
      "\t(('s014', 'susu'), 54)\n",
      "\t(('s015', '_language_independent'), 75)\n",
      "\t(('s015', 'francais'), 54)\n",
      "\t(('s015', 'maninka'), 54)\n",
      "\t(('s015', 'pular'), 54)\n",
      "\t(('s015', 'susu'), 54)\n",
      "\t(('s016', '_language_independent'), 75)\n",
      "\t(('s016', 'maninka'), 54)\n",
      "\t(('s016', 'susu'), 54)\n",
      "\t(('s017', '_language_independent'), 75)\n",
      "\t(('s017', 'francais'), 54)\n",
      "\t(('s017', 'susu'), 54)\n",
      "\t(('s018', '_language_independent'), 75)\n",
      "\t(('s018', 'francais'), 54)\n",
      "\t(('s018', 'pular'), 54)\n",
      "\t(('s018', 'susu'), 54)\n",
      "\t(('s019', '_language_independent'), 75)\n",
      "\t(('s019', 'francais'), 54)\n",
      "\t(('s019', 'maninka'), 54)\n",
      "\t(('s019', 'susu'), 54)\n",
      "\t(('s020', '_language_independent'), 75)\n",
      "\t(('s020', 'maninka'), 54)\n",
      "\t(('s021', '_language_independent'), 75)\n",
      "\t(('s021', 'francais'), 54)\n",
      "\t(('s021', 'maninka'), 54)\n",
      "\t(('s021', 'susu'), 36)\n",
      "\t(('s022', '_language_independent'), 75)\n",
      "\t(('s023', '_language_independent'), 75)\n",
      "\t(('s023', 'francais'), 60)\n",
      "\t(('s023', 'susu'), 54)\n",
      "\t(('s024', '_language_independent'), 75)\n",
      "\t(('s024', 'maninka'), 60)\n",
      "\t(('s024', 'susu'), 60)\n",
      "\t(('s025', '_language_independent'), 75)\n",
      "\t(('s025', 'francais'), 60)\n",
      "\t(('s025', 'pular'), 60)\n",
      "\t(('s026', 'francais'), 60)\n",
      "\t(('s026', 'maninka'), 60)\n",
      "\t(('s026', 'susu'), 60)\n",
      "\t(('s027', '_language_independent'), 300)\n",
      "\t(('s027', 'francais'), 60)\n",
      "\t(('s027', 'maninka'), 60)\n",
      "\t(('s027', 'susu'), 60)\n",
      "\t(('s028', '_language_independent'), 300)\n",
      "\t(('s028', 'francais'), 60)\n",
      "\t(('s028', 'pular'), 60)\n",
      "\t(('s028', 'susu'), 60)\n",
      "\t(('s029', 'francais'), 60)\n",
      "\t(('s029', 'maninka'), 60)\n",
      "\t(('s029', 'pular'), 60)\n",
      "\t(('s029', 'susu'), 60)\n",
      "\t(('s030', '_language_independent'), 75)\n",
      "\t(('s030', 'pular'), 60)\n",
      "\t(('s031', 'francais'), 60)\n",
      "\t(('s031', 'pular'), 57)\n",
      "\t(('s032', '_language_independent'), 75)\n",
      "\t(('s032', 'francais'), 60)\n",
      "\t(('s032', 'maninka'), 60)\n",
      "\t(('s032', 'pular'), 60)\n",
      "\t(('s032', 'susu'), 60)\n",
      "\t(('s033', '_language_independent'), 75)\n",
      "\t(('s033', 'francais'), 60)\n",
      "\t(('s033', 'maninka'), 60)\n",
      "\t(('s034', '_language_independent'), 75)\n",
      "\t(('s034', 'francais'), 60)\n",
      "\t(('s034', 'maninka'), 60)\n",
      "\t(('s034', 'susu'), 60)\n",
      "Fold 1 -- TEST\n",
      "\t(('s001', 'maninka'), 54)\n",
      "\t(('s003', '_language_independent'), 75)\n",
      "\t(('s003', 'maninka'), 54)\n",
      "\t(('s003', 'susu'), 54)\n",
      "\t(('s004', 'maninka'), 54)\n",
      "\t(('s005', '_language_independent'), 75)\n",
      "\t(('s005', 'susu'), 54)\n",
      "\t(('s006', '_language_independent'), 75)\n",
      "\t(('s006', 'maninka'), 54)\n",
      "\t(('s007', 'pular'), 54)\n",
      "\t(('s008', '_language_independent'), 75)\n",
      "\t(('s009', '_language_independent'), 75)\n",
      "\t(('s009', 'pular'), 54)\n",
      "\t(('s010', 'francais'), 54)\n",
      "\t(('s010', 'susu'), 54)\n",
      "\t(('s011', '_language_independent'), 75)\n",
      "\t(('s014', 'francais'), 54)\n",
      "\t(('s017', 'pular'), 54)\n",
      "\t(('s020', 'francais'), 54)\n",
      "\t(('s020', 'pular'), 54)\n",
      "\t(('s020', 'susu'), 54)\n",
      "\t(('s021', 'pular'), 54)\n",
      "\t(('s022', 'maninka'), 54)\n",
      "\t(('s022', 'susu'), 54)\n",
      "\t(('s023', 'maninka'), 60)\n",
      "\t(('s023', 'pular'), 60)\n",
      "\t(('s024', 'francais'), 60)\n",
      "\t(('s025', 'maninka'), 60)\n",
      "\t(('s025', 'susu'), 60)\n",
      "\t(('s026', '_language_independent'), 72)\n",
      "\t(('s027', 'pular'), 60)\n",
      "\t(('s028', 'maninka'), 60)\n",
      "\t(('s029', '_language_independent'), 150)\n",
      "\t(('s030', 'francais'), 60)\n",
      "\t(('s031', '_language_independent'), 75)\n",
      "\t(('s031', 'susu'), 60)\n",
      "\t(('s033', 'susu'), 60)\n",
      "---------------------\n",
      "Fold 2 -- TRAIN\n",
      "\t(('s001', '_language_independent'), 75)\n",
      "\t(('s001', 'maninka'), 54)\n",
      "\t(('s001', 'susu'), 54)\n",
      "\t(('s002', '_language_independent'), 75)\n",
      "\t(('s003', '_language_independent'), 75)\n",
      "\t(('s003', 'maninka'), 54)\n",
      "\t(('s004', '_language_independent'), 75)\n",
      "\t(('s004', 'maninka'), 54)\n",
      "\t(('s004', 'susu'), 54)\n",
      "\t(('s005', '_language_independent'), 75)\n",
      "\t(('s005', 'susu'), 54)\n",
      "\t(('s006', '_language_independent'), 75)\n",
      "\t(('s006', 'maninka'), 54)\n",
      "\t(('s007', 'pular'), 54)\n",
      "\t(('s007', 'susu'), 54)\n",
      "\t(('s008', '_language_independent'), 75)\n",
      "\t(('s008', 'maninka'), 54)\n",
      "\t(('s009', 'francais'), 54)\n",
      "\t(('s009', 'susu'), 54)\n",
      "\t(('s010', '_language_independent'), 75)\n",
      "\t(('s010', 'francais'), 54)\n",
      "\t(('s010', 'maninka'), 54)\n",
      "\t(('s010', 'pular'), 54)\n",
      "\t(('s010', 'susu'), 54)\n",
      "\t(('s011', '_language_independent'), 75)\n",
      "\t(('s011', 'susu'), 54)\n",
      "\t(('s012', 'francais'), 54)\n",
      "\t(('s013', 'susu'), 54)\n",
      "\t(('s014', '_language_independent'), 75)\n",
      "\t(('s015', 'francais'), 54)\n",
      "\t(('s015', 'susu'), 54)\n",
      "\t(('s016', '_language_independent'), 75)\n",
      "\t(('s017', '_language_independent'), 75)\n",
      "\t(('s017', 'pular'), 54)\n",
      "\t(('s018', '_language_independent'), 75)\n",
      "\t(('s018', 'francais'), 54)\n",
      "\t(('s018', 'pular'), 54)\n",
      "\t(('s018', 'susu'), 54)\n",
      "\t(('s019', '_language_independent'), 75)\n",
      "\t(('s019', 'francais'), 54)\n",
      "\t(('s019', 'susu'), 54)\n",
      "\t(('s020', '_language_independent'), 75)\n",
      "\t(('s020', 'francais'), 54)\n",
      "\t(('s020', 'maninka'), 54)\n",
      "\t(('s020', 'susu'), 54)\n",
      "\t(('s021', '_language_independent'), 75)\n",
      "\t(('s021', 'francais'), 54)\n",
      "\t(('s022', 'maninka'), 54)\n",
      "\t(('s023', '_language_independent'), 75)\n",
      "\t(('s023', 'francais'), 60)\n",
      "\t(('s023', 'pular'), 60)\n",
      "\t(('s023', 'susu'), 54)\n",
      "\t(('s024', '_language_independent'), 75)\n",
      "\t(('s024', 'francais'), 60)\n",
      "\t(('s024', 'maninka'), 60)\n",
      "\t(('s024', 'susu'), 60)\n",
      "\t(('s025', 'maninka'), 60)\n",
      "\t(('s025', 'susu'), 60)\n",
      "\t(('s026', '_language_independent'), 72)\n",
      "\t(('s026', 'maninka'), 60)\n",
      "\t(('s027', '_language_independent'), 300)\n",
      "\t(('s027', 'francais'), 60)\n",
      "\t(('s027', 'maninka'), 60)\n",
      "\t(('s027', 'pular'), 60)\n",
      "\t(('s028', '_language_independent'), 300)\n",
      "\t(('s028', 'francais'), 60)\n",
      "\t(('s028', 'maninka'), 60)\n",
      "\t(('s028', 'pular'), 60)\n",
      "\t(('s028', 'susu'), 60)\n",
      "\t(('s029', '_language_independent'), 150)\n",
      "\t(('s029', 'susu'), 60)\n",
      "\t(('s030', '_language_independent'), 75)\n",
      "\t(('s030', 'francais'), 60)\n",
      "\t(('s030', 'pular'), 60)\n",
      "\t(('s031', 'francais'), 60)\n",
      "\t(('s031', 'pular'), 57)\n",
      "\t(('s031', 'susu'), 60)\n",
      "\t(('s032', '_language_independent'), 75)\n",
      "\t(('s032', 'francais'), 60)\n",
      "\t(('s032', 'maninka'), 60)\n",
      "\t(('s032', 'pular'), 60)\n",
      "\t(('s032', 'susu'), 60)\n",
      "\t(('s033', 'francais'), 60)\n",
      "\t(('s033', 'maninka'), 60)\n",
      "\t(('s033', 'susu'), 60)\n",
      "\t(('s034', '_language_independent'), 75)\n",
      "\t(('s034', 'francais'), 60)\n",
      "\t(('s034', 'maninka'), 60)\n",
      "\t(('s034', 'susu'), 60)\n",
      "Fold 2 -- TEST\n",
      "\t(('s002', 'maninka'), 54)\n",
      "\t(('s003', 'susu'), 54)\n",
      "\t(('s007', '_language_independent'), 75)\n",
      "\t(('s009', '_language_independent'), 75)\n",
      "\t(('s009', 'pular'), 54)\n",
      "\t(('s012', '_language_independent'), 75)\n",
      "\t(('s012', 'susu'), 54)\n",
      "\t(('s013', '_language_independent'), 75)\n",
      "\t(('s014', 'francais'), 54)\n",
      "\t(('s014', 'maninka'), 54)\n",
      "\t(('s014', 'susu'), 54)\n",
      "\t(('s015', '_language_independent'), 75)\n",
      "\t(('s015', 'maninka'), 54)\n",
      "\t(('s015', 'pular'), 54)\n",
      "\t(('s016', 'maninka'), 54)\n",
      "\t(('s016', 'susu'), 54)\n",
      "\t(('s017', 'francais'), 54)\n",
      "\t(('s017', 'susu'), 54)\n",
      "\t(('s019', 'maninka'), 54)\n",
      "\t(('s020', 'pular'), 54)\n",
      "\t(('s021', 'maninka'), 54)\n",
      "\t(('s021', 'pular'), 54)\n",
      "\t(('s021', 'susu'), 36)\n",
      "\t(('s022', '_language_independent'), 75)\n",
      "\t(('s022', 'susu'), 54)\n",
      "\t(('s023', 'maninka'), 60)\n",
      "\t(('s025', '_language_independent'), 75)\n",
      "\t(('s025', 'francais'), 60)\n",
      "\t(('s025', 'pular'), 60)\n",
      "\t(('s026', 'francais'), 60)\n",
      "\t(('s026', 'susu'), 60)\n",
      "\t(('s027', 'susu'), 60)\n",
      "\t(('s029', 'francais'), 60)\n",
      "\t(('s029', 'maninka'), 60)\n",
      "\t(('s029', 'pular'), 60)\n",
      "\t(('s031', '_language_independent'), 75)\n",
      "\t(('s033', '_language_independent'), 75)\n",
      "---------------------\n",
      "Fold 3 -- TRAIN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t(('s002', 'maninka'), 54)\n",
      "\t(('s003', '_language_independent'), 75)\n",
      "\t(('s003', 'maninka'), 54)\n",
      "\t(('s004', '_language_independent'), 75)\n",
      "\t(('s004', 'maninka'), 54)\n",
      "\t(('s005', '_language_independent'), 75)\n",
      "\t(('s005', 'susu'), 54)\n",
      "\t(('s006', '_language_independent'), 75)\n",
      "\t(('s007', '_language_independent'), 75)\n",
      "\t(('s007', 'pular'), 54)\n",
      "\t(('s007', 'susu'), 54)\n",
      "\t(('s008', '_language_independent'), 75)\n",
      "\t(('s009', 'susu'), 54)\n",
      "\t(('s010', 'francais'), 54)\n",
      "\t(('s010', 'pular'), 54)\n",
      "\t(('s010', 'susu'), 54)\n",
      "\t(('s011', 'susu'), 54)\n",
      "\t(('s012', '_language_independent'), 75)\n",
      "\t(('s012', 'francais'), 54)\n",
      "\t(('s012', 'susu'), 54)\n",
      "\t(('s013', '_language_independent'), 75)\n",
      "\t(('s013', 'susu'), 54)\n",
      "\t(('s014', '_language_independent'), 75)\n",
      "\t(('s014', 'francais'), 54)\n",
      "\t(('s015', '_language_independent'), 75)\n",
      "\t(('s015', 'maninka'), 54)\n",
      "\t(('s015', 'pular'), 54)\n",
      "\t(('s016', '_language_independent'), 75)\n",
      "\t(('s016', 'maninka'), 54)\n",
      "\t(('s016', 'susu'), 54)\n",
      "\t(('s017', '_language_independent'), 75)\n",
      "\t(('s017', 'francais'), 54)\n",
      "\t(('s017', 'pular'), 54)\n",
      "\t(('s018', '_language_independent'), 75)\n",
      "\t(('s018', 'francais'), 54)\n",
      "\t(('s018', 'pular'), 54)\n",
      "\t(('s018', 'susu'), 54)\n",
      "\t(('s019', 'francais'), 54)\n",
      "\t(('s019', 'maninka'), 54)\n",
      "\t(('s019', 'susu'), 54)\n",
      "\t(('s020', 'francais'), 54)\n",
      "\t(('s020', 'susu'), 54)\n",
      "\t(('s021', '_language_independent'), 75)\n",
      "\t(('s021', 'maninka'), 54)\n",
      "\t(('s021', 'pular'), 54)\n",
      "\t(('s022', '_language_independent'), 75)\n",
      "\t(('s022', 'maninka'), 54)\n",
      "\t(('s023', '_language_independent'), 75)\n",
      "\t(('s023', 'maninka'), 60)\n",
      "\t(('s023', 'pular'), 60)\n",
      "\t(('s023', 'susu'), 54)\n",
      "\t(('s024', '_language_independent'), 75)\n",
      "\t(('s024', 'francais'), 60)\n",
      "\t(('s024', 'maninka'), 60)\n",
      "\t(('s025', '_language_independent'), 75)\n",
      "\t(('s025', 'francais'), 60)\n",
      "\t(('s025', 'maninka'), 60)\n",
      "\t(('s025', 'pular'), 60)\n",
      "\t(('s025', 'susu'), 60)\n",
      "\t(('s026', '_language_independent'), 72)\n",
      "\t(('s026', 'francais'), 60)\n",
      "\t(('s026', 'maninka'), 60)\n",
      "\t(('s027', '_language_independent'), 300)\n",
      "\t(('s027', 'francais'), 60)\n",
      "\t(('s027', 'pular'), 60)\n",
      "\t(('s027', 'susu'), 60)\n",
      "\t(('s028', 'francais'), 60)\n",
      "\t(('s028', 'maninka'), 60)\n",
      "\t(('s028', 'pular'), 60)\n",
      "\t(('s028', 'susu'), 60)\n",
      "\t(('s029', '_language_independent'), 150)\n",
      "\t(('s029', 'maninka'), 60)\n",
      "\t(('s029', 'pular'), 60)\n",
      "\t(('s029', 'susu'), 60)\n",
      "\t(('s030', 'pular'), 60)\n",
      "\t(('s031', '_language_independent'), 75)\n",
      "\t(('s031', 'francais'), 60)\n",
      "\t(('s031', 'pular'), 57)\n",
      "\t(('s031', 'susu'), 60)\n",
      "\t(('s032', '_language_independent'), 75)\n",
      "\t(('s032', 'francais'), 60)\n",
      "\t(('s032', 'maninka'), 60)\n",
      "\t(('s032', 'pular'), 60)\n",
      "\t(('s032', 'susu'), 60)\n",
      "\t(('s033', 'francais'), 60)\n",
      "\t(('s033', 'maninka'), 60)\n",
      "\t(('s034', '_language_independent'), 75)\n",
      "\t(('s034', 'francais'), 60)\n",
      "\t(('s034', 'susu'), 60)\n",
      "Fold 3 -- TEST\n",
      "\t(('s001', '_language_independent'), 75)\n",
      "\t(('s001', 'maninka'), 54)\n",
      "\t(('s001', 'susu'), 54)\n",
      "\t(('s002', '_language_independent'), 75)\n",
      "\t(('s003', 'susu'), 54)\n",
      "\t(('s004', 'susu'), 54)\n",
      "\t(('s006', 'maninka'), 54)\n",
      "\t(('s008', 'maninka'), 54)\n",
      "\t(('s009', '_language_independent'), 75)\n",
      "\t(('s009', 'francais'), 54)\n",
      "\t(('s009', 'pular'), 54)\n",
      "\t(('s010', '_language_independent'), 75)\n",
      "\t(('s010', 'maninka'), 54)\n",
      "\t(('s011', '_language_independent'), 75)\n",
      "\t(('s014', 'maninka'), 54)\n",
      "\t(('s014', 'susu'), 54)\n",
      "\t(('s015', 'francais'), 54)\n",
      "\t(('s015', 'susu'), 54)\n",
      "\t(('s017', 'susu'), 54)\n",
      "\t(('s019', '_language_independent'), 75)\n",
      "\t(('s020', '_language_independent'), 75)\n",
      "\t(('s020', 'maninka'), 54)\n",
      "\t(('s020', 'pular'), 54)\n",
      "\t(('s021', 'francais'), 54)\n",
      "\t(('s021', 'susu'), 36)\n",
      "\t(('s022', 'susu'), 54)\n",
      "\t(('s023', 'francais'), 60)\n",
      "\t(('s024', 'susu'), 60)\n",
      "\t(('s026', 'susu'), 60)\n",
      "\t(('s027', 'maninka'), 60)\n",
      "\t(('s028', '_language_independent'), 300)\n",
      "\t(('s029', 'francais'), 60)\n",
      "\t(('s030', '_language_independent'), 75)\n",
      "\t(('s030', 'francais'), 60)\n",
      "\t(('s033', '_language_independent'), 75)\n",
      "\t(('s033', 'susu'), 60)\n",
      "\t(('s034', 'maninka'), 60)\n",
      "---------------------\n",
      "Fold 4 -- TRAIN\n",
      "\t(('s001', 'susu'), 54)\n",
      "\t(('s002', 'maninka'), 54)\n",
      "\t(('s003', '_language_independent'), 75)\n",
      "\t(('s003', 'maninka'), 54)\n",
      "\t(('s003', 'susu'), 54)\n",
      "\t(('s004', 'susu'), 54)\n",
      "\t(('s005', '_language_independent'), 75)\n",
      "\t(('s005', 'susu'), 54)\n",
      "\t(('s006', '_language_independent'), 75)\n",
      "\t(('s006', 'maninka'), 54)\n",
      "\t(('s007', '_language_independent'), 75)\n",
      "\t(('s007', 'pular'), 54)\n",
      "\t(('s007', 'susu'), 54)\n",
      "\t(('s008', '_language_independent'), 75)\n",
      "\t(('s008', 'maninka'), 54)\n",
      "\t(('s009', '_language_independent'), 75)\n",
      "\t(('s009', 'pular'), 54)\n",
      "\t(('s009', 'susu'), 54)\n",
      "\t(('s010', '_language_independent'), 75)\n",
      "\t(('s010', 'francais'), 54)\n",
      "\t(('s010', 'maninka'), 54)\n",
      "\t(('s010', 'pular'), 54)\n",
      "\t(('s010', 'susu'), 54)\n",
      "\t(('s011', '_language_independent'), 75)\n",
      "\t(('s012', '_language_independent'), 75)\n",
      "\t(('s012', 'francais'), 54)\n",
      "\t(('s012', 'susu'), 54)\n",
      "\t(('s013', '_language_independent'), 75)\n",
      "\t(('s013', 'susu'), 54)\n",
      "\t(('s014', 'francais'), 54)\n",
      "\t(('s014', 'susu'), 54)\n",
      "\t(('s015', '_language_independent'), 75)\n",
      "\t(('s015', 'francais'), 54)\n",
      "\t(('s015', 'maninka'), 54)\n",
      "\t(('s015', 'pular'), 54)\n",
      "\t(('s016', '_language_independent'), 75)\n",
      "\t(('s016', 'susu'), 54)\n",
      "\t(('s017', '_language_independent'), 75)\n",
      "\t(('s017', 'susu'), 54)\n",
      "\t(('s018', 'francais'), 54)\n",
      "\t(('s018', 'pular'), 54)\n",
      "\t(('s019', 'susu'), 54)\n",
      "\t(('s020', '_language_independent'), 75)\n",
      "\t(('s020', 'francais'), 54)\n",
      "\t(('s020', 'maninka'), 54)\n",
      "\t(('s020', 'pular'), 54)\n",
      "\t(('s020', 'susu'), 54)\n",
      "\t(('s021', '_language_independent'), 75)\n",
      "\t(('s021', 'maninka'), 54)\n",
      "\t(('s021', 'pular'), 54)\n",
      "\t(('s022', '_language_independent'), 75)\n",
      "\t(('s022', 'maninka'), 54)\n",
      "\t(('s023', 'francais'), 60)\n",
      "\t(('s023', 'maninka'), 60)\n",
      "\t(('s023', 'pular'), 60)\n",
      "\t(('s023', 'susu'), 54)\n",
      "\t(('s024', '_language_independent'), 75)\n",
      "\t(('s024', 'francais'), 60)\n",
      "\t(('s024', 'maninka'), 60)\n",
      "\t(('s024', 'susu'), 60)\n",
      "\t(('s025', '_language_independent'), 75)\n",
      "\t(('s025', 'francais'), 60)\n",
      "\t(('s025', 'maninka'), 60)\n",
      "\t(('s025', 'pular'), 60)\n",
      "\t(('s025', 'susu'), 60)\n",
      "\t(('s026', 'francais'), 60)\n",
      "\t(('s026', 'maninka'), 60)\n",
      "\t(('s026', 'susu'), 60)\n",
      "\t(('s027', '_language_independent'), 300)\n",
      "\t(('s027', 'francais'), 60)\n",
      "\t(('s027', 'maninka'), 60)\n",
      "\t(('s027', 'susu'), 60)\n",
      "\t(('s028', '_language_independent'), 300)\n",
      "\t(('s028', 'francais'), 60)\n",
      "\t(('s028', 'maninka'), 60)\n",
      "\t(('s028', 'susu'), 60)\n",
      "\t(('s029', '_language_independent'), 150)\n",
      "\t(('s029', 'francais'), 60)\n",
      "\t(('s029', 'susu'), 60)\n",
      "\t(('s030', '_language_independent'), 75)\n",
      "\t(('s030', 'francais'), 60)\n",
      "\t(('s032', '_language_independent'), 75)\n",
      "\t(('s032', 'francais'), 60)\n",
      "\t(('s032', 'maninka'), 60)\n",
      "\t(('s032', 'pular'), 60)\n",
      "\t(('s033', '_language_independent'), 75)\n",
      "\t(('s033', 'francais'), 60)\n",
      "\t(('s033', 'susu'), 60)\n",
      "\t(('s034', 'susu'), 60)\n",
      "Fold 4 -- TEST\n",
      "\t(('s001', '_language_independent'), 75)\n",
      "\t(('s001', 'maninka'), 54)\n",
      "\t(('s002', '_language_independent'), 75)\n",
      "\t(('s004', '_language_independent'), 75)\n",
      "\t(('s004', 'maninka'), 54)\n",
      "\t(('s009', 'francais'), 54)\n",
      "\t(('s011', 'susu'), 54)\n",
      "\t(('s014', '_language_independent'), 75)\n",
      "\t(('s014', 'maninka'), 54)\n",
      "\t(('s015', 'susu'), 54)\n",
      "\t(('s016', 'maninka'), 54)\n",
      "\t(('s017', 'francais'), 54)\n",
      "\t(('s017', 'pular'), 54)\n",
      "\t(('s018', '_language_independent'), 75)\n",
      "\t(('s018', 'susu'), 54)\n",
      "\t(('s019', '_language_independent'), 75)\n",
      "\t(('s019', 'francais'), 54)\n",
      "\t(('s019', 'maninka'), 54)\n",
      "\t(('s021', 'francais'), 54)\n",
      "\t(('s021', 'susu'), 36)\n",
      "\t(('s022', 'susu'), 54)\n",
      "\t(('s023', '_language_independent'), 75)\n",
      "\t(('s026', '_language_independent'), 72)\n",
      "\t(('s027', 'pular'), 60)\n",
      "\t(('s028', 'pular'), 60)\n",
      "\t(('s029', 'maninka'), 60)\n",
      "\t(('s029', 'pular'), 60)\n",
      "\t(('s030', 'pular'), 60)\n",
      "\t(('s031', '_language_independent'), 75)\n",
      "\t(('s031', 'francais'), 60)\n",
      "\t(('s031', 'pular'), 57)\n",
      "\t(('s031', 'susu'), 60)\n",
      "\t(('s032', 'susu'), 60)\n",
      "\t(('s033', 'maninka'), 60)\n",
      "\t(('s034', '_language_independent'), 75)\n",
      "\t(('s034', 'francais'), 60)\n",
      "\t(('s034', 'maninka'), 60)\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for fold_index in range(FOLD_COUNT):\n",
    "    train_records = records_per_fold[fold_index][\"train_records\"]\n",
    "    test_records = records_per_fold[fold_index][\"test_records\"]\n",
    "\n",
    "    print(f\"Fold {fold_index} -- TRAIN\")\n",
    "    _ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(train_records, ['speaker_id', 'language'])) if r[1]>0]\n",
    "\n",
    "    print(f\"Fold {fold_index} -- TEST\")\n",
    "    _ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(test_records, ['speaker_id', 'language'])) if r[1]>0]\n",
    "    \n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 -- TRAIN: (6018)\n",
      "\t(('_language_independent',), 2400)\n",
      "\t(('francais',), 738)\n",
      "\t(('maninka',), 960)\n",
      "\t(('pular',), 852)\n",
      "\t(('susu',), 1068)\n",
      "Fold 0 -- TEST: (2241)\n",
      "\t(('_language_independent',), 672)\n",
      "\t(('francais',), 522)\n",
      "\t(('maninka',), 396)\n",
      "\t(('pular',), 57)\n",
      "\t(('susu',), 594)\n",
      "---------------------\n",
      "Fold 1 -- TRAIN: (5940)\n",
      "\t(('_language_independent',), 2325)\n",
      "\t(('francais',), 978)\n",
      "\t(('maninka',), 906)\n",
      "\t(('pular',), 519)\n",
      "\t(('susu',), 1212)\n",
      "Fold 1 -- TEST: (2319)\n",
      "\t(('_language_independent',), 747)\n",
      "\t(('francais',), 282)\n",
      "\t(('maninka',), 450)\n",
      "\t(('pular',), 390)\n",
      "\t(('susu',), 450)\n",
      "---------------------\n",
      "Fold 2 -- TRAIN: (6036)\n",
      "\t(('_language_independent',), 2397)\n",
      "\t(('francais',), 972)\n",
      "\t(('maninka',), 912)\n",
      "\t(('pular',), 573)\n",
      "\t(('susu',), 1182)\n",
      "Fold 2 -- TEST: (2223)\n",
      "\t(('_language_independent',), 675)\n",
      "\t(('francais',), 288)\n",
      "\t(('maninka',), 444)\n",
      "\t(('pular',), 336)\n",
      "\t(('susu',), 480)\n",
      "---------------------\n",
      "Fold 3 -- TRAIN: (5796)\n",
      "\t(('_language_independent',), 2097)\n",
      "\t(('francais',), 918)\n",
      "\t(('maninka',), 912)\n",
      "\t(('pular',), 801)\n",
      "\t(('susu',), 1068)\n",
      "Fold 3 -- TEST: (2463)\n",
      "\t(('_language_independent',), 975)\n",
      "\t(('francais',), 342)\n",
      "\t(('maninka',), 444)\n",
      "\t(('pular',), 108)\n",
      "\t(('susu',), 594)\n",
      "---------------------\n",
      "Fold 4 -- TRAIN: (6003)\n",
      "\t(('_language_independent',), 2325)\n",
      "\t(('francais',), 924)\n",
      "\t(('maninka',), 906)\n",
      "\t(('pular',), 558)\n",
      "\t(('susu',), 1290)\n",
      "Fold 4 -- TEST: (2256)\n",
      "\t(('_language_independent',), 747)\n",
      "\t(('francais',), 336)\n",
      "\t(('maninka',), 450)\n",
      "\t(('pular',), 351)\n",
      "\t(('susu',), 372)\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for fold_index in range(FOLD_COUNT):\n",
    "    train_records = records_per_fold[fold_index][\"train_records\"]\n",
    "    test_records = records_per_fold[fold_index][\"test_records\"]\n",
    "\n",
    "    print(f\"Fold {fold_index} -- TRAIN: ({len(train_records)})\")\n",
    "    _ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(train_records, ['language'])) if r[1]>0]\n",
    "\n",
    "    print(f\"Fold {fold_index} -- TEST: ({len(test_records)})\")\n",
    "    _ = [print(f\"\\t{r}\") for r in sorted(count_by_attribute(test_records, ['language'])) if r[1]>0]\n",
    "    \n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(records, feature_name):\n",
    "    features_list = []\n",
    "    \n",
    "    features_input_dir = BASE_DIR / feature_name\n",
    "\n",
    "    for r in records:\n",
    "        feature_file_name = r['file'].replace(\".wav\", \".h5context\")\n",
    "        feature_path = Path(features_input_dir) / feature_file_name\n",
    "        with h5py.File(feature_path, 'r') as f:\n",
    "            features_shape = f['info'][1:].astype(int)\n",
    "            features = np.array(f['features'][:]).reshape(features_shape)\n",
    "            \n",
    "            padded_features = np.zeros((MAX_FEATURE_SEQUENCE_LENGTH, 512), dtype=features.dtype)\n",
    "            padded_features[:features_shape[0], :] = features\n",
    "            \n",
    "            \n",
    "            features_list.append(padded_features)\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_category_labels(records):\n",
    "    bias_category_labels = {}\n",
    "    \n",
    "    for cat in bias_categories:\n",
    "        for cat_val in bias_categories[cat]:\n",
    "            bias_category_labels[f\"{cat}__{cat_val}\"] = [1 if r[cat]==cat_val else 0 for r in records]\n",
    "            \n",
    "    return bias_category_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRCNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 conv_pooling_type, \n",
    "                 conv_dropout_p, \n",
    "                 fc_dropout_p, \n",
    "                 voice_cmd_neuron_count, \n",
    "                 voice_cmd_lng_neuron_count,\n",
    "                 objective_type\n",
    "                ):\n",
    "        \n",
    "        super(ASRCNN, self).__init__()\n",
    "        \n",
    "        if conv_pooling_type not in {\"max\", \"avg\"}:\n",
    "            raise ValueError(f\"Unknown Conv Pooling Type: {conv_pooling_type}\")\n",
    "            \n",
    "        conv_pooling_class_by_type = {\n",
    "            \"max\": nn.MaxPool1d,\n",
    "            \"avg\": nn.AvgPool1d,\n",
    "        }\n",
    "        \n",
    "        conv_pooling_class = conv_pooling_class_by_type[conv_pooling_type]\n",
    "        \n",
    "        self.objective_type = objective_type\n",
    "        \n",
    "        self.conv0 = nn.Conv1d(in_channels=512, out_channels=8, kernel_size=1)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3)\n",
    "        self.drop1 = nn.Dropout(p=conv_dropout_p)\n",
    "        self.pool1 = conv_pooling_class(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3)\n",
    "        self.drop2 = nn.Dropout(p=conv_dropout_p)\n",
    "        self.pool2 = conv_pooling_class(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.drop3 = nn.Dropout(p=conv_dropout_p)\n",
    "        self.pool3 = conv_pooling_class(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.drop4 = nn.Dropout(p=conv_dropout_p)\n",
    "        self.pool4 = conv_pooling_class(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.drop5 = nn.Dropout(p=fc_dropout_p)\n",
    "        \n",
    "        self.lin61 = nn.Linear(in_features=112, out_features=voice_cmd_neuron_count)\n",
    "        \n",
    "        # 'voice_cmd', 'voice_cmd__and__voice_cmd_lng'\n",
    "        if self.objective_type == 'voice_cmd__and__voice_cmd_lng':\n",
    "            self.lin62 = nn.Linear(in_features=112, out_features=voice_cmd_lng_neuron_count)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv0(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        v1 = torch.mean(x, dim=2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        v2 = torch.mean(x, dim=2)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.drop4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        v3 = torch.mean(x, dim=2)\n",
    "        \n",
    "        v = torch.cat((v1, v2, v3), axis=1)\n",
    "        v = self.drop5(v)\n",
    "        \n",
    "        if self.objective_type == 'voice_cmd':\n",
    "            logits_voice_cmd = self.lin61(v)\n",
    "            return logits_voice_cmd\n",
    "        elif self.objective_type == 'voice_cmd__and__voice_cmd_lng':\n",
    "            logits_voice_cmd = self.lin61(v)\n",
    "            logits_voice_cmd_lng = self.lin62(v)\n",
    "            return logits_voice_cmd, logits_voice_cmd_lng\n",
    "        else:\n",
    "            raise(f\"Unknown objective type: {self.objective_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_fold(fold_id, feature_name):\n",
    "    \n",
    "    train_records = records_per_fold[fold_index][\"train_records\"]\n",
    "    test_records = records_per_fold[fold_index][\"test_records\"]\n",
    "    \n",
    "    train_features = load_features(train_records, feature_name)\n",
    "    test_features = load_features(test_records, feature_name)\n",
    "    \n",
    "    train_x = np.array(train_features)\n",
    "    test_x = np.array(test_features)\n",
    "    \n",
    "    train_y = {}\n",
    "    train_y['voice_cmd'] = np.array([voice_cmd_class_id_by_name[r['label']] for r in train_records])\n",
    "    train_y['voice_cmd_lng'] = np.array([voice_cmd_lng_class_id_by_name[r['language']] for r in train_records])\n",
    "    train_y['spkr_mothertongue'] = np.array([spkr_mothertongue_class_id_by_name[r['speaker_mothertongue']] for r in train_records])\n",
    "    train_y['spkr_gender'] = np.array([spkr_gender_class_id_by_name[r['speaker_gender']] for r in train_records])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    test_y = {}\n",
    "    test_y['voice_cmd'] = np.array([voice_cmd_class_id_by_name[r['label']] for r in test_records])\n",
    "    test_y['voice_cmd_lng'] = np.array([voice_cmd_lng_class_id_by_name[r['language']] for r in test_records])\n",
    "    test_y['spkr_mothertongue'] = np.array([spkr_mothertongue_class_id_by_name[r['speaker_mothertongue']] for r in test_records])\n",
    "    test_y['spkr_gender'] = np.array([spkr_gender_class_id_by_name[r['speaker_gender']] for r in test_records])\n",
    "\n",
    "    train_bias_category_labels = get_bias_category_labels(train_records)\n",
    "    test_bias_category_labels = get_bias_category_labels(test_records)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, train_bias_category_labels, test_bias_category_labels\n",
    "\n",
    "    \n",
    "def get_loaders_for_fold(fold_id, feature_name, batch_size):\n",
    "    \n",
    "    train_x, train_y, test_x, test_y, train_bias_category_labels, test_bias_category_labels = \\\n",
    "        get_data_for_fold(fold_id, feature_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x), \n",
    "        torch.tensor(train_y['voice_cmd']),\n",
    "        torch.tensor(train_y['voice_cmd_lng']),\n",
    "        # torch.tensor(train_y['spkr_mothertongue']),\n",
    "        # torch.tensor(train_y['spkr_gender']),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_x), \n",
    "        torch.tensor(test_y['voice_cmd']),\n",
    "        torch.tensor(test_y['voice_cmd_lng']),\n",
    "        # torch.tensor(test_y['spkr_mothertongue']),\n",
    "        # torch.tensor(test_y['spkr_gender']),\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader, train_bias_category_labels, test_bias_category_labels\n",
    "\n",
    "\n",
    "def get_predictions_for_logits(logits):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return torch.argmax(probs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, objective_type, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (x, y_voice_cmd, y_voice_cmd_lng) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y_voice_cmd = y_voice_cmd.to(device)\n",
    "        y_voice_cmd_lng = y_voice_cmd_lng.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "\n",
    "        if objective_type == 'voice_cmd':\n",
    "            logits_voice_cmd = outputs\n",
    "            loss = criterion(logits_voice_cmd, y_voice_cmd)\n",
    "        elif objective_type == 'voice_cmd__and__voice_cmd_lng':\n",
    "            logits_voice_cmd, logits_voice_cmd_lng = outputs    \n",
    "            loss = (criterion(logits_voice_cmd, y_voice_cmd) + criterion(logits_voice_cmd_lng, y_voice_cmd_lng)) / 2\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown objective type: {objective_type}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "\n",
    "def test(model, criterion, objective_type, loader, bias_category_labels):\n",
    "    model.eval()\n",
    "    accumulated_loss = 0\n",
    "\n",
    "    pred_classes = []\n",
    "    true_classes = []\n",
    "\n",
    "    pred_classes_lng = []\n",
    "    true_classes_lng = []\n",
    "\n",
    "    for batch_idx, (x, y_voice_cmd, y_voice_cmd_lng) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y_voice_cmd = y_voice_cmd.to(device)\n",
    "        y_voice_cmd_lng = y_voice_cmd_lng.to(device)\n",
    "\n",
    "        outputs = model(x)\n",
    "\n",
    "        if objective_type == 'voice_cmd':\n",
    "            logits_voice_cmd = outputs\n",
    "\n",
    "            pred_classes.extend(\n",
    "                get_predictions_for_logits(logits_voice_cmd).cpu().numpy()\n",
    "            )\n",
    "            true_classes.extend(y_voice_cmd.cpu().numpy())\n",
    "\n",
    "            loss = criterion(logits_voice_cmd, y_voice_cmd)\n",
    "        elif objective_type == 'voice_cmd__and__voice_cmd_lng':\n",
    "            logits_voice_cmd, logits_voice_cmd_lng = outputs\n",
    "            pred_classes.extend(\n",
    "                get_predictions_for_logits(logits_voice_cmd).cpu().numpy()\n",
    "            )\n",
    "            true_classes.extend(y_voice_cmd.cpu().numpy())\n",
    "\n",
    "            pred_classes_lng.extend(\n",
    "                get_predictions_for_logits(logits_voice_cmd_lng).cpu().numpy()\n",
    "            )\n",
    "            true_classes_lng.extend(y_voice_cmd_lng.cpu().numpy())\n",
    "\n",
    "            loss = (criterion(logits_voice_cmd, y_voice_cmd) + criterion(logits_voice_cmd_lng, y_voice_cmd_lng)) /2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown objective type: {objective_type}\")\n",
    "\n",
    "        accumulated_loss += loss.item()\n",
    "\n",
    "    n = len(true_classes)\n",
    "\n",
    "    average_loss = accumulated_loss/n\n",
    "    \n",
    "    acc = sklearn.metrics.accuracy_score(true_classes, pred_classes)\n",
    "    acc_by_bais_category = {\n",
    "        category: sklearn.metrics.accuracy_score(true_classes, pred_classes, sample_weight=sw)\n",
    "        for category, sw in bias_category_labels.items()\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if objective_type == 'voice_cmd__and__voice_cmd_lng':\n",
    "        acc_lng = sklearn.metrics.accuracy_score(true_classes_lng, pred_classes_lng)\n",
    "        acc_by_bais_category_lng = {\n",
    "            category: sklearn.metrics.accuracy_score(true_classes_lng, pred_classes_lng, sample_weight=sw)\n",
    "            for category, sw in bias_category_labels.items()\n",
    "        }\n",
    "    else:\n",
    "        acc_lng = -1\n",
    "        acc_by_bais_category_lng = {\n",
    "            category: -1\n",
    "            for category, sw in bias_category_labels.items()\n",
    "        }\n",
    "        \n",
    "    return n, average_loss, acc, acc_by_bais_category, acc_lng, acc_by_bais_category_lng\n",
    "      \n",
    "        \n",
    "def train_on_fold(model, fold_id, feature_name, objective_type, batch_size, epochs):\n",
    "    torch.manual_seed(0)\n",
    "    results = {}\n",
    "    \n",
    "    train_loader, test_loader, train_bias_category_labels, test_bias_category_labels = get_loaders_for_fold(fold_id, feature_name, batch_size)\n",
    "\n",
    "    print(summary(model, torch.zeros((10, MAX_FEATURE_SEQUENCE_LENGTH, 512)).to(device), show_input=False))\n",
    "    print(f\"train_n: {len(train_loader.dataset)}\")\n",
    "    print(f\"test_n: {len(test_loader.dataset)}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        # train on training set\n",
    "        train(model, optimizer, criterion, objective_type, train_loader)\n",
    "        \n",
    "        # test on training set\n",
    "        train_n, train_average_loss, train_acc, train_acc_by_bais_category, train_acc_lng, train_acc_by_bais_category_lng = \\\n",
    "            test(model, criterion, objective_type, train_loader, train_bias_category_labels)\n",
    "        \n",
    "        # test on test set\n",
    "        test_n, test_average_loss, test_acc, test_acc_by_bais_category, test_acc_lng, test_acc_by_bais_category_lng = \\\n",
    "            test(model, criterion, objective_type, test_loader, test_bias_category_labels)\n",
    "        \n",
    "\n",
    "        if epoch%10==0:\n",
    "            print(f\"Epoch: {epoch}. Train Loss: {train_average_loss:0.4}. Test Loss: {test_average_loss:0.4}. Train Acc: {train_acc:0.4}. Test Acc:{test_acc:0.4}\")\n",
    "        \n",
    "         \n",
    "        results[epoch] = {\n",
    "            'epoch': epoch,\n",
    "            \n",
    "            'train_n': train_n,\n",
    "            'train_loss': train_average_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_acc_lng': train_acc_lng,\n",
    "            \n",
    "            'test_n': test_n,\n",
    "            'test_loss': test_average_loss,\n",
    "            'test_acc': test_acc,\n",
    "            'test_acc_lng': test_acc_lng\n",
    "        }\n",
    "        \n",
    "        for c in train_acc_by_bais_category:\n",
    "            results[epoch][f\"train_acc_{c}\"] = train_acc_by_bais_category[c]\n",
    "            results[epoch][f\"train_n_{c}\"] = int(np.sum(train_bias_category_labels[c]))\n",
    "            \n",
    "        for c in train_acc_by_bais_category_lng:\n",
    "            results[epoch][f\"train_acc_lng_{c}\"] = train_acc_by_bais_category_lng[c]\n",
    "            \n",
    "            \n",
    "        for c in test_acc_by_bais_category:\n",
    "            results[epoch][f\"test_acc_{c}\"] = test_acc_by_bais_category[c]\n",
    "            results[epoch][f\"test_n_{c}\"] = int(np.sum(test_bias_category_labels[c]))\n",
    "\n",
    "        for c in test_acc_by_bais_category_lng:\n",
    "            results[epoch][f\"test_acc_lng_{c}\"] = test_acc_by_bais_category_lng[c]\n",
    "            \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def save_results(model_name, all_folds_results):\n",
    "    for result_entry in all_folds_results:\n",
    "        feature_name = result_entry['feature_name']\n",
    "        fold_index = result_entry['fold_index']\n",
    "        \n",
    "        Path(RESULTS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "        fname = f\"{RESULTS_DIR}/{model_name}/{feature_name}_{fold_index}.csv\"\n",
    "        Path(fname).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(fname, 'w') as f:\n",
    "            fieldnames = sorted(result_entry['epochs'][1].keys())\n",
    "            \n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='raise')\n",
    "            \n",
    "            writer.writeheader()\n",
    "            \n",
    "            for epoch in sorted(result_entry['epochs'].keys()):\n",
    "                writer.writerow(result_entry['epochs'][epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd')\n",
      "(0, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(0, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd')\n",
      "(0, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(0, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd')\n",
      "(0, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(0, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd')\n",
      "(0, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(1, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd')\n",
      "(1, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(1, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd')\n",
      "(1, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(1, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd')\n",
      "(1, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(1, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd')\n",
      "(1, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(2, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd')\n",
      "(2, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(2, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd')\n",
      "(2, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(2, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd')\n",
      "(2, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(2, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd')\n",
      "(2, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(3, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd')\n",
      "(3, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(3, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd')\n",
      "(3, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(3, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd')\n",
      "(3, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(3, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd')\n",
      "(3, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(4, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd')\n",
      "(4, 'avg', 0.3, 0.3, 'wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(4, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd')\n",
      "(4, 'avg', 0.3, 0.3, 'wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "(4, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd')\n",
      "(4, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-c', 'voice_cmd__and__voice_cmd_lng')\n",
      "(4, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd')\n",
      "(4, 'avg', 0.3, 0.3, 'retrained-wav2vec_features-z', 'voice_cmd__and__voice_cmd_lng')\n",
      "ASRCNN__conv_pool_avg__conv_dp_0.3__fc_dp_0.3__fn_wav2vec_features-c__obj_voice_cmd using wav2vec_features-c on fold#0\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv1d-1        [10, 8, 200]           4,104           4,104\n",
      "          Conv1d-2        [10, 8, 198]             200             200\n",
      "         Dropout-3        [10, 8, 198]               0               0\n",
      "       AvgPool1d-4         [10, 8, 99]               0               0\n",
      "          Conv1d-5        [10, 16, 97]             400             400\n",
      "         Dropout-6        [10, 16, 97]               0               0\n",
      "       AvgPool1d-7        [10, 16, 48]               0               0\n",
      "          Conv1d-8        [10, 32, 46]           1,568           1,568\n",
      "         Dropout-9        [10, 32, 46]               0               0\n",
      "      AvgPool1d-10        [10, 32, 23]               0               0\n",
      "         Conv1d-11        [10, 64, 21]           6,208           6,208\n",
      "        Dropout-12        [10, 64, 21]               0               0\n",
      "      AvgPool1d-13        [10, 64, 10]               0               0\n",
      "        Dropout-14           [10, 112]               0               0\n",
      "         Linear-15           [10, 105]          11,865          11,865\n",
      "=======================================================================\n",
      "Total params: 24,345\n",
      "Trainable params: 24,345\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "train_n: 6003\n",
      "test_n: 2256\n",
      "Epoch: 10. Train Loss: 3.828. Test Loss: 4.003. Train Acc: 0.09529. Test Acc:0.06826\n",
      "Epoch: 20. Train Loss: 3.026. Test Loss: 3.289. Train Acc: 0.2462. Test Acc:0.1764\n",
      "Epoch: 30. Train Loss: 2.496. Test Loss: 2.827. Train Acc: 0.351. Test Acc:0.2606\n",
      "Epoch: 40. Train Loss: 2.112. Test Loss: 2.502. Train Acc: 0.4476. Test Acc:0.3391\n",
      "Epoch: 50. Train Loss: 1.79. Test Loss: 2.216. Train Acc: 0.5296. Test Acc:0.4109\n",
      "Epoch: 60. Train Loss: 1.531. Test Loss: 1.994. Train Acc: 0.5994. Test Acc:0.4623\n",
      "Epoch: 70. Train Loss: 1.318. Test Loss: 1.816. Train Acc: 0.663. Test Acc:0.5084\n",
      "Epoch: 80. Train Loss: 1.154. Test Loss: 1.686. Train Acc: 0.7096. Test Acc:0.5594\n",
      "Epoch: 90. Train Loss: 1.049. Test Loss: 1.593. Train Acc: 0.7346. Test Acc:0.5833\n",
      "Epoch: 100. Train Loss: 0.9515. Test Loss: 1.513. Train Acc: 0.762. Test Acc:0.6121\n",
      "Epoch: 110. Train Loss: 0.8826. Test Loss: 1.458. Train Acc: 0.7793. Test Acc:0.6237\n",
      "Epoch: 120. Train Loss: 0.8171. Test Loss: 1.41. Train Acc: 0.8004. Test Acc:0.6427\n",
      "Epoch: 130. Train Loss: 0.7565. Test Loss: 1.347. Train Acc: 0.8144. Test Acc:0.6569\n",
      "Epoch: 140. Train Loss: 0.7054. Test Loss: 1.292. Train Acc: 0.8339. Test Acc:0.6698\n",
      "Epoch: 150. Train Loss: 0.6634. Test Loss: 1.247. Train Acc: 0.8431. Test Acc:0.6809\n",
      "Epoch: 160. Train Loss: 0.6291. Test Loss: 1.229. Train Acc: 0.8517. Test Acc:0.6871\n",
      "Epoch: 170. Train Loss: 0.5864. Test Loss: 1.17. Train Acc: 0.8631. Test Acc:0.707\n",
      "Epoch: 180. Train Loss: 0.5525. Test Loss: 1.152. Train Acc: 0.8709. Test Acc:0.7101\n",
      "Epoch: 190. Train Loss: 0.5258. Test Loss: 1.099. Train Acc: 0.8757. Test Acc:0.7301\n",
      "Epoch: 200. Train Loss: 0.491. Test Loss: 1.081. Train Acc: 0.8847. Test Acc:0.7358\n",
      "Epoch: 210. Train Loss: 0.4687. Test Loss: 1.068. Train Acc: 0.8911. Test Acc:0.7363\n",
      "Epoch: 220. Train Loss: 0.4478. Test Loss: 1.047. Train Acc: 0.8969. Test Acc:0.7496\n",
      "Epoch: 230. Train Loss: 0.4252. Test Loss: 1.029. Train Acc: 0.9027. Test Acc:0.7566\n",
      "Epoch: 240. Train Loss: 0.4104. Test Loss: 1.032. Train Acc: 0.9089. Test Acc:0.7482\n",
      "Epoch: 250. Train Loss: 0.3962. Test Loss: 1.016. Train Acc: 0.9099. Test Acc:0.7522\n",
      "Epoch: 260. Train Loss: 0.3845. Test Loss: 1.018. Train Acc: 0.9149. Test Acc:0.7544\n",
      "Epoch: 270. Train Loss: 0.3677. Test Loss: 1.011. Train Acc: 0.9187. Test Acc:0.754\n",
      "Epoch: 280. Train Loss: 0.3567. Test Loss: 1.006. Train Acc: 0.9179. Test Acc:0.7549\n",
      "Epoch: 290. Train Loss: 0.3481. Test Loss: 0.9924. Train Acc: 0.9235. Test Acc:0.7611\n",
      "Epoch: 300. Train Loss: 0.3313. Test Loss: 0.9864. Train Acc: 0.9269. Test Acc:0.7611\n",
      "Epoch: 310. Train Loss: 0.3236. Test Loss: 0.9742. Train Acc: 0.9284. Test Acc:0.7651\n",
      "Epoch: 320. Train Loss: 0.3264. Test Loss: 0.9836. Train Acc: 0.9252. Test Acc:0.7629\n",
      "Epoch: 330. Train Loss: 0.3043. Test Loss: 0.9753. Train Acc: 0.932. Test Acc:0.7713\n",
      "Epoch: 340. Train Loss: 0.2968. Test Loss: 0.9662. Train Acc: 0.9312. Test Acc:0.7739\n",
      "Epoch: 350. Train Loss: 0.2982. Test Loss: 0.9839. Train Acc: 0.9312. Test Acc:0.7713\n",
      "Epoch: 360. Train Loss: 0.2933. Test Loss: 0.9763. Train Acc: 0.9307. Test Acc:0.7686\n",
      "Epoch: 370. Train Loss: 0.2846. Test Loss: 0.9643. Train Acc: 0.9325. Test Acc:0.7748\n",
      "Epoch: 380. Train Loss: 0.2733. Test Loss: 0.9758. Train Acc: 0.933. Test Acc:0.7717\n",
      "Epoch: 390. Train Loss: 0.275. Test Loss: 0.9769. Train Acc: 0.9337. Test Acc:0.7744\n",
      "Epoch: 400. Train Loss: 0.2733. Test Loss: 0.982. Train Acc: 0.9354. Test Acc:0.7713\n",
      "Epoch: 410. Train Loss: 0.2594. Test Loss: 0.9608. Train Acc: 0.9365. Test Acc:0.7806\n",
      "Epoch: 420. Train Loss: 0.2441. Test Loss: 0.9426. Train Acc: 0.9414. Test Acc:0.7797\n",
      "Epoch: 430. Train Loss: 0.2392. Test Loss: 0.9567. Train Acc: 0.9424. Test Acc:0.7788\n",
      "Epoch: 440. Train Loss: 0.2301. Test Loss: 0.9652. Train Acc: 0.9462. Test Acc:0.7797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450. Train Loss: 0.2308. Test Loss: 0.9717. Train Acc: 0.9444. Test Acc:0.7801\n",
      "Epoch: 460. Train Loss: 0.2198. Test Loss: 0.9497. Train Acc: 0.947. Test Acc:0.7837\n",
      "Epoch: 470. Train Loss: 0.2144. Test Loss: 0.9593. Train Acc: 0.9487. Test Acc:0.7868\n",
      "Epoch: 480. Train Loss: 0.213. Test Loss: 0.9644. Train Acc: 0.9485. Test Acc:0.7837\n",
      "Epoch: 490. Train Loss: 0.2081. Test Loss: 0.9616. Train Acc: 0.951. Test Acc:0.7841\n",
      "Epoch: 500. Train Loss: 0.2055. Test Loss: 0.9781. Train Acc: 0.9515. Test Acc:0.7846\n",
      "Epoch: 510. Train Loss: 0.1981. Test Loss: 0.9724. Train Acc: 0.951. Test Acc:0.785\n",
      "Epoch: 520. Train Loss: 0.1966. Test Loss: 0.9699. Train Acc: 0.9517. Test Acc:0.7859\n",
      "Epoch: 530. Train Loss: 0.1883. Test Loss: 0.9507. Train Acc: 0.9539. Test Acc:0.7859\n",
      "Epoch: 540. Train Loss: 0.1838. Test Loss: 0.9556. Train Acc: 0.9557. Test Acc:0.7877\n",
      "Epoch: 550. Train Loss: 0.185. Test Loss: 0.9629. Train Acc: 0.9564. Test Acc:0.7917\n",
      "Epoch: 560. Train Loss: 0.1845. Test Loss: 0.9622. Train Acc: 0.953. Test Acc:0.7881\n",
      "Epoch: 570. Train Loss: 0.1844. Test Loss: 0.9707. Train Acc: 0.954. Test Acc:0.7886\n",
      "Epoch: 580. Train Loss: 0.178. Test Loss: 0.9568. Train Acc: 0.9545. Test Acc:0.7948\n",
      "Epoch: 590. Train Loss: 0.171. Test Loss: 0.9548. Train Acc: 0.9572. Test Acc:0.7863\n",
      "Epoch: 600. Train Loss: 0.1673. Test Loss: 0.958. Train Acc: 0.9582. Test Acc:0.789\n",
      "Epoch: 610. Train Loss: 0.1612. Test Loss: 0.9729. Train Acc: 0.9612. Test Acc:0.7863\n",
      "Epoch: 620. Train Loss: 0.1596. Test Loss: 0.9987. Train Acc: 0.9614. Test Acc:0.7859\n",
      "Epoch: 630. Train Loss: 0.1616. Test Loss: 1.001. Train Acc: 0.9602. Test Acc:0.7788\n",
      "Epoch: 640. Train Loss: 0.1583. Test Loss: 0.9903. Train Acc: 0.9602. Test Acc:0.785\n",
      "Epoch: 650. Train Loss: 0.1506. Test Loss: 0.9771. Train Acc: 0.9627. Test Acc:0.793\n",
      "Epoch: 660. Train Loss: 0.154. Test Loss: 0.9682. Train Acc: 0.9624. Test Acc:0.7832\n",
      "Epoch: 670. Train Loss: 0.1526. Test Loss: 0.9756. Train Acc: 0.9624. Test Acc:0.7881\n",
      "Epoch: 680. Train Loss: 0.1707. Test Loss: 1.034. Train Acc: 0.9549. Test Acc:0.7713\n",
      "Epoch: 690. Train Loss: 0.1595. Test Loss: 1.001. Train Acc: 0.9587. Test Acc:0.7762\n",
      "Epoch: 700. Train Loss: 0.1585. Test Loss: 1.008. Train Acc: 0.9572. Test Acc:0.7797\n",
      "Epoch: 710. Train Loss: 0.1472. Test Loss: 0.9937. Train Acc: 0.9614. Test Acc:0.7828\n",
      "Epoch: 720. Train Loss: 0.1524. Test Loss: 1.025. Train Acc: 0.9595. Test Acc:0.7779\n",
      "Epoch: 730. Train Loss: 0.1417. Test Loss: 0.978. Train Acc: 0.9635. Test Acc:0.785\n",
      "Epoch: 740. Train Loss: 0.1414. Test Loss: 1.001. Train Acc: 0.9625. Test Acc:0.7841\n",
      "Epoch: 750. Train Loss: 0.1282. Test Loss: 0.9912. Train Acc: 0.9677. Test Acc:0.7895\n",
      "Epoch: 760. Train Loss: 0.1267. Test Loss: 0.989. Train Acc: 0.969. Test Acc:0.7899\n",
      "Epoch: 770. Train Loss: 0.1282. Test Loss: 0.993. Train Acc: 0.9667. Test Acc:0.7895\n",
      "Epoch: 780. Train Loss: 0.1263. Test Loss: 1.0. Train Acc: 0.9673. Test Acc:0.785\n",
      "Epoch: 790. Train Loss: 0.1367. Test Loss: 1.016. Train Acc: 0.9637. Test Acc:0.7837\n",
      "Epoch: 800. Train Loss: 0.1189. Test Loss: 0.9752. Train Acc: 0.9697. Test Acc:0.7921\n",
      "Epoch: 810. Train Loss: 0.1184. Test Loss: 0.9973. Train Acc: 0.9693. Test Acc:0.7939\n",
      "Epoch: 820. Train Loss: 0.1263. Test Loss: 0.9734. Train Acc: 0.9682. Test Acc:0.7899\n",
      "Epoch: 830. Train Loss: 0.117. Test Loss: 0.9924. Train Acc: 0.9718. Test Acc:0.7979\n",
      "Epoch: 840. Train Loss: 0.1162. Test Loss: 1.007. Train Acc: 0.9705. Test Acc:0.7886\n",
      "Epoch: 850. Train Loss: 0.1156. Test Loss: 0.9881. Train Acc: 0.9715. Test Acc:0.7921\n",
      "Epoch: 860. Train Loss: 0.1092. Test Loss: 0.9923. Train Acc: 0.9727. Test Acc:0.7877\n",
      "Epoch: 870. Train Loss: 0.1065. Test Loss: 0.9674. Train Acc: 0.9728. Test Acc:0.8014\n",
      "Epoch: 880. Train Loss: 0.1114. Test Loss: 1.036. Train Acc: 0.9713. Test Acc:0.7881\n",
      "Epoch: 890. Train Loss: 0.1057. Test Loss: 0.9836. Train Acc: 0.9727. Test Acc:0.7957\n",
      "Epoch: 900. Train Loss: 0.1021. Test Loss: 0.9954. Train Acc: 0.9742. Test Acc:0.7881\n",
      "Epoch: 910. Train Loss: 0.1031. Test Loss: 1.006. Train Acc: 0.974. Test Acc:0.7939\n",
      "Epoch: 920. Train Loss: 0.1034. Test Loss: 1.005. Train Acc: 0.9737. Test Acc:0.7979\n",
      "Epoch: 930. Train Loss: 0.1005. Test Loss: 1.01. Train Acc: 0.975. Test Acc:0.7961\n",
      "Epoch: 940. Train Loss: 0.09628. Test Loss: 1.004. Train Acc: 0.9768. Test Acc:0.7952\n",
      "Epoch: 950. Train Loss: 0.1039. Test Loss: 1.011. Train Acc: 0.9743. Test Acc:0.797\n",
      "Epoch: 960. Train Loss: 0.09934. Test Loss: 1.016. Train Acc: 0.9758. Test Acc:0.7868\n",
      "Epoch: 970. Train Loss: 0.09893. Test Loss: 1.016. Train Acc: 0.9727. Test Acc:0.7903\n",
      "Epoch: 980. Train Loss: 0.0896. Test Loss: 1.015. Train Acc: 0.9772. Test Acc:0.7912\n",
      "Epoch: 990. Train Loss: 0.09943. Test Loss: 1.009. Train Acc: 0.9728. Test Acc:0.785\n",
      "Epoch: 1000. Train Loss: 0.09176. Test Loss: 1.019. Train Acc: 0.9763. Test Acc:0.7926\n",
      "ASRCNN__conv_pool_avg__conv_dp_0.3__fc_dp_0.3__fn_wav2vec_features-c__obj_voice_cmd__and__voice_cmd_lng using wav2vec_features-c on fold#0\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv1d-1        [10, 8, 200]           4,104           4,104\n",
      "          Conv1d-2        [10, 8, 198]             200             200\n",
      "         Dropout-3        [10, 8, 198]               0               0\n",
      "       AvgPool1d-4         [10, 8, 99]               0               0\n",
      "          Conv1d-5        [10, 16, 97]             400             400\n",
      "         Dropout-6        [10, 16, 97]               0               0\n",
      "       AvgPool1d-7        [10, 16, 48]               0               0\n",
      "          Conv1d-8        [10, 32, 46]           1,568           1,568\n",
      "         Dropout-9        [10, 32, 46]               0               0\n",
      "      AvgPool1d-10        [10, 32, 23]               0               0\n",
      "         Conv1d-11        [10, 64, 21]           6,208           6,208\n",
      "        Dropout-12        [10, 64, 21]               0               0\n",
      "      AvgPool1d-13        [10, 64, 10]               0               0\n",
      "        Dropout-14           [10, 112]               0               0\n",
      "         Linear-15           [10, 105]          11,865          11,865\n",
      "         Linear-16             [10, 5]             565             565\n",
      "=======================================================================\n",
      "Total params: 24,910\n",
      "Trainable params: 24,910\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "train_n: 6003\n",
      "test_n: 2256\n",
      "Epoch: 10. Train Loss: 2.405. Test Loss: 2.592. Train Acc: 0.1404. Test Acc:0.09309\n",
      "Epoch: 20. Train Loss: 2.037. Test Loss: 2.285. Train Acc: 0.2665. Test Acc:0.1928\n",
      "Epoch: 30. Train Loss: 1.755. Test Loss: 2.023. Train Acc: 0.395. Test Acc:0.293\n",
      "Epoch: 40. Train Loss: 1.528. Test Loss: 1.83. Train Acc: 0.4958. Test Acc:0.3692\n",
      "Epoch: 50. Train Loss: 1.378. Test Loss: 1.714. Train Acc: 0.5602. Test Acc:0.4131\n",
      "Epoch: 60. Train Loss: 1.265. Test Loss: 1.633. Train Acc: 0.6079. Test Acc:0.4437\n",
      "Epoch: 70. Train Loss: 1.169. Test Loss: 1.574. Train Acc: 0.6405. Test Acc:0.4716\n",
      "Epoch: 80. Train Loss: 1.084. Test Loss: 1.513. Train Acc: 0.6732. Test Acc:0.4973\n",
      "Epoch: 90. Train Loss: 1.004. Test Loss: 1.458. Train Acc: 0.7056. Test Acc:0.5208\n",
      "Epoch: 100. Train Loss: 0.9289. Test Loss: 1.38. Train Acc: 0.7328. Test Acc:0.5501\n",
      "Epoch: 110. Train Loss: 0.8815. Test Loss: 1.363. Train Acc: 0.7493. Test Acc:0.5567\n",
      "Epoch: 120. Train Loss: 0.8273. Test Loss: 1.299. Train Acc: 0.7766. Test Acc:0.5833\n",
      "Epoch: 130. Train Loss: 0.7833. Test Loss: 1.294. Train Acc: 0.7891. Test Acc:0.5882\n",
      "Epoch: 140. Train Loss: 0.7484. Test Loss: 1.25. Train Acc: 0.8021. Test Acc:0.6121\n",
      "Epoch: 150. Train Loss: 0.7168. Test Loss: 1.19. Train Acc: 0.8158. Test Acc:0.633\n",
      "Epoch: 160. Train Loss: 0.6992. Test Loss: 1.232. Train Acc: 0.8201. Test Acc:0.6241\n",
      "Epoch: 170. Train Loss: 0.6698. Test Loss: 1.172. Train Acc: 0.8261. Test Acc:0.6387\n",
      "Epoch: 180. Train Loss: 0.6362. Test Loss: 1.178. Train Acc: 0.8429. Test Acc:0.6472\n",
      "Epoch: 190. Train Loss: 0.613. Test Loss: 1.136. Train Acc: 0.8509. Test Acc:0.6569\n",
      "Epoch: 200. Train Loss: 0.5844. Test Loss: 1.097. Train Acc: 0.8636. Test Acc:0.6751\n",
      "Epoch: 210. Train Loss: 0.5813. Test Loss: 1.102. Train Acc: 0.8609. Test Acc:0.6711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220. Train Loss: 0.5521. Test Loss: 1.082. Train Acc: 0.8686. Test Acc:0.6804\n",
      "Epoch: 230. Train Loss: 0.53. Test Loss: 1.079. Train Acc: 0.8802. Test Acc:0.6848\n",
      "Epoch: 240. Train Loss: 0.5215. Test Loss: 1.067. Train Acc: 0.8837. Test Acc:0.6862\n",
      "Epoch: 250. Train Loss: 0.4994. Test Loss: 1.058. Train Acc: 0.8907. Test Acc:0.6955\n",
      "Epoch: 260. Train Loss: 0.4905. Test Loss: 1.023. Train Acc: 0.8927. Test Acc:0.6999\n",
      "Epoch: 270. Train Loss: 0.4767. Test Loss: 1.026. Train Acc: 0.8961. Test Acc:0.7066\n",
      "Epoch: 280. Train Loss: 0.4699. Test Loss: 1.055. Train Acc: 0.9. Test Acc:0.7008\n",
      "Epoch: 290. Train Loss: 0.464. Test Loss: 1.025. Train Acc: 0.8997. Test Acc:0.6986\n",
      "Epoch: 300. Train Loss: 0.4547. Test Loss: 1.029. Train Acc: 0.8971. Test Acc:0.7026\n",
      "Epoch: 310. Train Loss: 0.4426. Test Loss: 1.034. Train Acc: 0.9017. Test Acc:0.707\n",
      "Epoch: 320. Train Loss: 0.4304. Test Loss: 1.014. Train Acc: 0.9072. Test Acc:0.7101\n",
      "Epoch: 330. Train Loss: 0.4224. Test Loss: 1.026. Train Acc: 0.9119. Test Acc:0.7021\n",
      "Epoch: 340. Train Loss: 0.4065. Test Loss: 0.9864. Train Acc: 0.9167. Test Acc:0.7194\n",
      "Epoch: 350. Train Loss: 0.4104. Test Loss: 0.9836. Train Acc: 0.911. Test Acc:0.7185\n",
      "Epoch: 360. Train Loss: 0.4005. Test Loss: 0.9965. Train Acc: 0.915. Test Acc:0.7137\n",
      "Epoch: 370. Train Loss: 0.3999. Test Loss: 1.021. Train Acc: 0.9149. Test Acc:0.7101\n",
      "Epoch: 380. Train Loss: 0.3877. Test Loss: 1.001. Train Acc: 0.9182. Test Acc:0.7128\n",
      "Epoch: 390. Train Loss: 0.3913. Test Loss: 0.984. Train Acc: 0.9125. Test Acc:0.7185\n",
      "Epoch: 400. Train Loss: 0.3865. Test Loss: 1.04. Train Acc: 0.9185. Test Acc:0.7021\n",
      "Epoch: 410. Train Loss: 0.3784. Test Loss: 0.9925. Train Acc: 0.9169. Test Acc:0.7088\n",
      "Epoch: 420. Train Loss: 0.3706. Test Loss: 0.9968. Train Acc: 0.919. Test Acc:0.7145\n",
      "Epoch: 430. Train Loss: 0.3608. Test Loss: 0.9662. Train Acc: 0.9255. Test Acc:0.7283\n",
      "Epoch: 440. Train Loss: 0.3488. Test Loss: 0.9725. Train Acc: 0.9294. Test Acc:0.723\n",
      "Epoch: 450. Train Loss: 0.3456. Test Loss: 0.9697. Train Acc: 0.9285. Test Acc:0.7287\n",
      "Epoch: 460. Train Loss: 0.3446. Test Loss: 0.9497. Train Acc: 0.9282. Test Acc:0.7256\n",
      "Epoch: 470. Train Loss: 0.3325. Test Loss: 0.9935. Train Acc: 0.9345. Test Acc:0.7234\n",
      "Epoch: 480. Train Loss: 0.327. Test Loss: 0.9657. Train Acc: 0.936. Test Acc:0.7296\n",
      "Epoch: 490. Train Loss: 0.3225. Test Loss: 0.965. Train Acc: 0.9389. Test Acc:0.7332\n",
      "Epoch: 500. Train Loss: 0.3161. Test Loss: 0.9732. Train Acc: 0.9357. Test Acc:0.7309\n",
      "Epoch: 510. Train Loss: 0.3105. Test Loss: 0.9418. Train Acc: 0.94. Test Acc:0.7407\n",
      "Epoch: 520. Train Loss: 0.3081. Test Loss: 0.974. Train Acc: 0.9387. Test Acc:0.7296\n",
      "Epoch: 530. Train Loss: 0.3056. Test Loss: 0.9638. Train Acc: 0.9425. Test Acc:0.7349\n",
      "Epoch: 540. Train Loss: 0.3006. Test Loss: 0.9546. Train Acc: 0.9405. Test Acc:0.7318\n",
      "Epoch: 550. Train Loss: 0.2985. Test Loss: 0.9288. Train Acc: 0.9417. Test Acc:0.7332\n",
      "Epoch: 560. Train Loss: 0.2944. Test Loss: 0.9542. Train Acc: 0.9405. Test Acc:0.7411\n",
      "Epoch: 570. Train Loss: 0.2886. Test Loss: 0.949. Train Acc: 0.942. Test Acc:0.727\n",
      "Epoch: 580. Train Loss: 0.2822. Test Loss: 0.923. Train Acc: 0.9437. Test Acc:0.7349\n",
      "Epoch: 590. Train Loss: 0.282. Test Loss: 0.967. Train Acc: 0.9452. Test Acc:0.7376\n",
      "Epoch: 600. Train Loss: 0.28. Test Loss: 0.9611. Train Acc: 0.9442. Test Acc:0.7376\n",
      "Epoch: 610. Train Loss: 0.2771. Test Loss: 0.9349. Train Acc: 0.945. Test Acc:0.7416\n",
      "Epoch: 620. Train Loss: 0.271. Test Loss: 0.9366. Train Acc: 0.9437. Test Acc:0.7349\n",
      "Epoch: 630. Train Loss: 0.2745. Test Loss: 0.9482. Train Acc: 0.9405. Test Acc:0.7429\n",
      "Epoch: 640. Train Loss: 0.2664. Test Loss: 0.9543. Train Acc: 0.9457. Test Acc:0.7394\n",
      "Epoch: 650. Train Loss: 0.2597. Test Loss: 0.9484. Train Acc: 0.9469. Test Acc:0.7389\n",
      "Epoch: 660. Train Loss: 0.2571. Test Loss: 0.9471. Train Acc: 0.9474. Test Acc:0.7473\n",
      "Epoch: 670. Train Loss: 0.2605. Test Loss: 0.9797. Train Acc: 0.9439. Test Acc:0.7292\n",
      "Epoch: 680. Train Loss: 0.2549. Test Loss: 0.9372. Train Acc: 0.9499. Test Acc:0.7416\n",
      "Epoch: 690. Train Loss: 0.2498. Test Loss: 0.9557. Train Acc: 0.9505. Test Acc:0.7487\n",
      "Epoch: 700. Train Loss: 0.2444. Test Loss: 0.9375. Train Acc: 0.9499. Test Acc:0.7504\n",
      "Epoch: 710. Train Loss: 0.2459. Test Loss: 0.9825. Train Acc: 0.9494. Test Acc:0.7438\n",
      "Epoch: 720. Train Loss: 0.2431. Test Loss: 0.9446. Train Acc: 0.9497. Test Acc:0.7416\n",
      "Epoch: 730. Train Loss: 0.2378. Test Loss: 0.9344. Train Acc: 0.9512. Test Acc:0.7478\n",
      "Epoch: 740. Train Loss: 0.2433. Test Loss: 0.9422. Train Acc: 0.9472. Test Acc:0.7442\n",
      "Epoch: 750. Train Loss: 0.2308. Test Loss: 0.9342. Train Acc: 0.9524. Test Acc:0.75\n",
      "Epoch: 760. Train Loss: 0.2259. Test Loss: 0.9482. Train Acc: 0.9537. Test Acc:0.7496\n",
      "Epoch: 770. Train Loss: 0.2284. Test Loss: 0.9647. Train Acc: 0.9542. Test Acc:0.7416\n",
      "Epoch: 780. Train Loss: 0.2201. Test Loss: 0.9348. Train Acc: 0.9554. Test Acc:0.7504\n",
      "Epoch: 790. Train Loss: 0.2277. Test Loss: 0.9863. Train Acc: 0.9524. Test Acc:0.7402\n",
      "Epoch: 800. Train Loss: 0.2408. Test Loss: 0.9801. Train Acc: 0.948. Test Acc:0.7465\n",
      "Epoch: 810. Train Loss: 0.2153. Test Loss: 0.9472. Train Acc: 0.9559. Test Acc:0.7562\n",
      "Epoch: 820. Train Loss: 0.2132. Test Loss: 0.9327. Train Acc: 0.9562. Test Acc:0.7544\n",
      "Epoch: 830. Train Loss: 0.2115. Test Loss: 0.9206. Train Acc: 0.9549. Test Acc:0.7509\n",
      "Epoch: 840. Train Loss: 0.2122. Test Loss: 0.9522. Train Acc: 0.9525. Test Acc:0.7522\n",
      "Epoch: 850. Train Loss: 0.2049. Test Loss: 0.9267. Train Acc: 0.958. Test Acc:0.7562\n",
      "Epoch: 860. Train Loss: 0.2151. Test Loss: 0.9357. Train Acc: 0.9549. Test Acc:0.7518\n",
      "Epoch: 870. Train Loss: 0.1997. Test Loss: 0.9107. Train Acc: 0.9584. Test Acc:0.754\n",
      "Epoch: 880. Train Loss: 0.2008. Test Loss: 0.9321. Train Acc: 0.957. Test Acc:0.7487\n",
      "Epoch: 890. Train Loss: 0.2031. Test Loss: 0.9809. Train Acc: 0.9579. Test Acc:0.7442\n",
      "Epoch: 900. Train Loss: 0.1958. Test Loss: 0.9039. Train Acc: 0.9592. Test Acc:0.7651\n",
      "Epoch: 910. Train Loss: 0.1965. Test Loss: 0.9114. Train Acc: 0.9577. Test Acc:0.7575\n",
      "Epoch: 920. Train Loss: 0.1946. Test Loss: 0.9339. Train Acc: 0.958. Test Acc:0.7558\n",
      "Epoch: 930. Train Loss: 0.1917. Test Loss: 0.93. Train Acc: 0.9595. Test Acc:0.7535\n",
      "Epoch: 940. Train Loss: 0.1919. Test Loss: 0.9521. Train Acc: 0.9634. Test Acc:0.7513\n",
      "Epoch: 950. Train Loss: 0.1945. Test Loss: 0.9586. Train Acc: 0.9569. Test Acc:0.7527\n",
      "Epoch: 960. Train Loss: 0.1831. Test Loss: 0.926. Train Acc: 0.9627. Test Acc:0.7558\n",
      "Epoch: 970. Train Loss: 0.1881. Test Loss: 0.9235. Train Acc: 0.9604. Test Acc:0.7575\n",
      "Epoch: 980. Train Loss: 0.1936. Test Loss: 0.9245. Train Acc: 0.9585. Test Acc:0.7535\n",
      "Epoch: 990. Train Loss: 0.1871. Test Loss: 0.9627. Train Acc: 0.96. Test Acc:0.7438\n",
      "Epoch: 1000. Train Loss: 0.1863. Test Loss: 0.9394. Train Acc: 0.9617. Test Acc:0.7496\n",
      "ASRCNN__conv_pool_avg__conv_dp_0.3__fc_dp_0.3__fn_wav2vec_features-z__obj_voice_cmd using wav2vec_features-z on fold#0\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Conv1d-1        [10, 8, 200]           4,104           4,104\n",
      "          Conv1d-2        [10, 8, 198]             200             200\n",
      "         Dropout-3        [10, 8, 198]               0               0\n",
      "       AvgPool1d-4         [10, 8, 99]               0               0\n",
      "          Conv1d-5        [10, 16, 97]             400             400\n",
      "         Dropout-6        [10, 16, 97]               0               0\n",
      "       AvgPool1d-7        [10, 16, 48]               0               0\n",
      "          Conv1d-8        [10, 32, 46]           1,568           1,568\n",
      "         Dropout-9        [10, 32, 46]               0               0\n",
      "      AvgPool1d-10        [10, 32, 23]               0               0\n",
      "         Conv1d-11        [10, 64, 21]           6,208           6,208\n",
      "        Dropout-12        [10, 64, 21]               0               0\n",
      "      AvgPool1d-13        [10, 64, 10]               0               0\n",
      "        Dropout-14           [10, 112]               0               0\n",
      "         Linear-15           [10, 105]          11,865          11,865\n",
      "=======================================================================\n",
      "Total params: 24,345\n",
      "Trainable params: 24,345\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "train_n: 6003\n",
      "test_n: 2256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Train Loss: 4.066. Test Loss: 4.189. Train Acc: 0.0583. Test Acc:0.043\n",
      "Epoch: 20. Train Loss: 3.322. Test Loss: 3.525. Train Acc: 0.1543. Test Acc:0.1082\n",
      "Epoch: 30. Train Loss: 2.967. Test Loss: 3.224. Train Acc: 0.2482. Test Acc:0.1724\n",
      "Epoch: 40. Train Loss: 2.612. Test Loss: 2.932. Train Acc: 0.3295. Test Acc:0.2473\n",
      "Epoch: 50. Train Loss: 2.305. Test Loss: 2.693. Train Acc: 0.4088. Test Acc:0.2979\n",
      "Epoch: 60. Train Loss: 2.069. Test Loss: 2.52. Train Acc: 0.4701. Test Acc:0.34\n",
      "Epoch: 70. Train Loss: 1.886. Test Loss: 2.366. Train Acc: 0.5174. Test Acc:0.3865\n",
      "Epoch: 80. Train Loss: 1.724. Test Loss: 2.258. Train Acc: 0.5616. Test Acc:0.414\n",
      "Epoch: 90. Train Loss: 1.597. Test Loss: 2.156. Train Acc: 0.598. Test Acc:0.4384\n",
      "Epoch: 100. Train Loss: 1.477. Test Loss: 2.05. Train Acc: 0.6289. Test Acc:0.4672\n",
      "Epoch: 110. Train Loss: 1.37. Test Loss: 1.97. Train Acc: 0.6587. Test Acc:0.4894\n",
      "Epoch: 120. Train Loss: 1.279. Test Loss: 1.906. Train Acc: 0.6792. Test Acc:0.5075\n",
      "Epoch: 130. Train Loss: 1.189. Test Loss: 1.817. Train Acc: 0.7033. Test Acc:0.5253\n",
      "Epoch: 140. Train Loss: 1.113. Test Loss: 1.75. Train Acc: 0.726. Test Acc:0.5443\n",
      "Epoch: 150. Train Loss: 1.05. Test Loss: 1.687. Train Acc: 0.7418. Test Acc:0.5656\n",
      "Epoch: 160. Train Loss: 0.975. Test Loss: 1.601. Train Acc: 0.763. Test Acc:0.5847\n",
      "Epoch: 170. Train Loss: 0.9188. Test Loss: 1.553. Train Acc: 0.7791. Test Acc:0.6002\n",
      "Epoch: 180. Train Loss: 0.8606. Test Loss: 1.493. Train Acc: 0.7959. Test Acc:0.6179\n",
      "Epoch: 190. Train Loss: 0.8276. Test Loss: 1.446. Train Acc: 0.8054. Test Acc:0.6303\n",
      "Epoch: 200. Train Loss: 0.7783. Test Loss: 1.376. Train Acc: 0.8126. Test Acc:0.6476\n",
      "Epoch: 210. Train Loss: 0.7438. Test Loss: 1.341. Train Acc: 0.8219. Test Acc:0.6578\n",
      "Epoch: 220. Train Loss: 0.7117. Test Loss: 1.308. Train Acc: 0.8339. Test Acc:0.6693\n",
      "Epoch: 230. Train Loss: 0.678. Test Loss: 1.288. Train Acc: 0.8392. Test Acc:0.6791\n",
      "Epoch: 240. Train Loss: 0.6572. Test Loss: 1.253. Train Acc: 0.8444. Test Acc:0.6902\n",
      "Epoch: 250. Train Loss: 0.6302. Test Loss: 1.217. Train Acc: 0.8501. Test Acc:0.6919\n",
      "Epoch: 260. Train Loss: 0.6127. Test Loss: 1.189. Train Acc: 0.8547. Test Acc:0.6999\n",
      "Epoch: 270. Train Loss: 0.574. Test Loss: 1.17. Train Acc: 0.8661. Test Acc:0.7026\n",
      "Epoch: 280. Train Loss: 0.5507. Test Loss: 1.157. Train Acc: 0.8701. Test Acc:0.7097\n",
      "Epoch: 290. Train Loss: 0.5399. Test Loss: 1.146. Train Acc: 0.8731. Test Acc:0.7172\n",
      "Epoch: 300. Train Loss: 0.52. Test Loss: 1.115. Train Acc: 0.8777. Test Acc:0.7216\n",
      "Epoch: 310. Train Loss: 0.5006. Test Loss: 1.104. Train Acc: 0.8834. Test Acc:0.715\n",
      "Epoch: 320. Train Loss: 0.4786. Test Loss: 1.103. Train Acc: 0.8889. Test Acc:0.7278\n",
      "Epoch: 330. Train Loss: 0.4632. Test Loss: 1.09. Train Acc: 0.8912. Test Acc:0.7278\n",
      "Epoch: 340. Train Loss: 0.4555. Test Loss: 1.071. Train Acc: 0.8922. Test Acc:0.7349\n",
      "Epoch: 350. Train Loss: 0.4457. Test Loss: 1.064. Train Acc: 0.8934. Test Acc:0.7363\n",
      "Epoch: 360. Train Loss: 0.4249. Test Loss: 1.054. Train Acc: 0.901. Test Acc:0.7434\n",
      "Epoch: 370. Train Loss: 0.4181. Test Loss: 1.048. Train Acc: 0.9. Test Acc:0.746\n",
      "Epoch: 380. Train Loss: 0.4116. Test Loss: 1.045. Train Acc: 0.901. Test Acc:0.7487\n",
      "Epoch: 390. Train Loss: 0.4089. Test Loss: 1.038. Train Acc: 0.902. Test Acc:0.7473\n",
      "Epoch: 400. Train Loss: 0.3991. Test Loss: 1.024. Train Acc: 0.9017. Test Acc:0.7491\n",
      "Epoch: 410. Train Loss: 0.3795. Test Loss: 1.03. Train Acc: 0.9089. Test Acc:0.7535\n",
      "Epoch: 420. Train Loss: 0.3717. Test Loss: 1.015. Train Acc: 0.9117. Test Acc:0.7535\n",
      "Epoch: 430. Train Loss: 0.3631. Test Loss: 1.015. Train Acc: 0.9157. Test Acc:0.7589\n",
      "Epoch: 440. Train Loss: 0.3647. Test Loss: 1.007. Train Acc: 0.913. Test Acc:0.7584\n",
      "Epoch: 450. Train Loss: 0.3596. Test Loss: 1.013. Train Acc: 0.9109. Test Acc:0.7584\n",
      "Epoch: 460. Train Loss: 0.3475. Test Loss: 1.0. Train Acc: 0.9169. Test Acc:0.7668\n",
      "Epoch: 470. Train Loss: 0.3466. Test Loss: 0.9936. Train Acc: 0.9139. Test Acc:0.7664\n",
      "Epoch: 480. Train Loss: 0.3427. Test Loss: 0.9924. Train Acc: 0.9175. Test Acc:0.7655\n",
      "Epoch: 490. Train Loss: 0.338. Test Loss: 0.982. Train Acc: 0.9184. Test Acc:0.7699\n",
      "Epoch: 500. Train Loss: 0.3354. Test Loss: 1.003. Train Acc: 0.9165. Test Acc:0.7637\n",
      "Epoch: 510. Train Loss: 0.3176. Test Loss: 0.979. Train Acc: 0.9212. Test Acc:0.7757\n",
      "Epoch: 520. Train Loss: 0.3062. Test Loss: 0.9966. Train Acc: 0.9284. Test Acc:0.7682\n",
      "Epoch: 530. Train Loss: 0.3097. Test Loss: 0.9758. Train Acc: 0.9247. Test Acc:0.7775\n",
      "Epoch: 540. Train Loss: 0.3062. Test Loss: 0.9641. Train Acc: 0.9269. Test Acc:0.7824\n",
      "Epoch: 550. Train Loss: 0.2936. Test Loss: 0.9743. Train Acc: 0.931. Test Acc:0.7784\n",
      "Epoch: 560. Train Loss: 0.2933. Test Loss: 0.9742. Train Acc: 0.9294. Test Acc:0.7788\n",
      "Epoch: 570. Train Loss: 0.2895. Test Loss: 0.9603. Train Acc: 0.9275. Test Acc:0.7832\n",
      "Epoch: 580. Train Loss: 0.2839. Test Loss: 0.9582. Train Acc: 0.9319. Test Acc:0.781\n",
      "Epoch: 590. Train Loss: 0.2711. Test Loss: 0.9622. Train Acc: 0.9369. Test Acc:0.7819\n",
      "Epoch: 600. Train Loss: 0.2782. Test Loss: 0.9538. Train Acc: 0.9322. Test Acc:0.7855\n",
      "Epoch: 610. Train Loss: 0.2683. Test Loss: 0.9493. Train Acc: 0.9355. Test Acc:0.7886\n",
      "Epoch: 620. Train Loss: 0.2698. Test Loss: 0.9495. Train Acc: 0.9349. Test Acc:0.7917\n",
      "Epoch: 630. Train Loss: 0.2553. Test Loss: 0.9478. Train Acc: 0.9409. Test Acc:0.7872\n",
      "Epoch: 640. Train Loss: 0.2571. Test Loss: 0.9565. Train Acc: 0.9379. Test Acc:0.789\n",
      "Epoch: 650. Train Loss: 0.2567. Test Loss: 0.9454. Train Acc: 0.938. Test Acc:0.7912\n",
      "Epoch: 660. Train Loss: 0.2465. Test Loss: 0.949. Train Acc: 0.9424. Test Acc:0.7908\n",
      "Epoch: 670. Train Loss: 0.2509. Test Loss: 0.9413. Train Acc: 0.9414. Test Acc:0.7903\n",
      "Epoch: 680. Train Loss: 0.2394. Test Loss: 0.951. Train Acc: 0.9437. Test Acc:0.7846\n",
      "Epoch: 690. Train Loss: 0.2344. Test Loss: 0.9394. Train Acc: 0.9454. Test Acc:0.7903\n",
      "Epoch: 700. Train Loss: 0.2341. Test Loss: 0.9362. Train Acc: 0.9442. Test Acc:0.7948\n",
      "Epoch: 710. Train Loss: 0.2307. Test Loss: 0.9519. Train Acc: 0.9462. Test Acc:0.7859\n",
      "Epoch: 720. Train Loss: 0.2272. Test Loss: 0.951. Train Acc: 0.945. Test Acc:0.7943\n",
      "Epoch: 730. Train Loss: 0.2263. Test Loss: 0.9496. Train Acc: 0.9459. Test Acc:0.7895\n",
      "Epoch: 740. Train Loss: 0.2208. Test Loss: 0.9444. Train Acc: 0.9479. Test Acc:0.7926\n",
      "Epoch: 750. Train Loss: 0.2202. Test Loss: 0.9409. Train Acc: 0.947. Test Acc:0.7917\n",
      "Epoch: 760. Train Loss: 0.2175. Test Loss: 0.9507. Train Acc: 0.9475. Test Acc:0.7921\n",
      "Epoch: 770. Train Loss: 0.2192. Test Loss: 0.9323. Train Acc: 0.9469. Test Acc:0.7921\n"
     ]
    }
   ],
   "source": [
    "trial_params = list(\n",
    "    itertools.product(\n",
    "        range(FOLD_COUNT),\n",
    "        CONV_POOLING_TYPES,\n",
    "        CONV_DROPOUT_PROBABILITIES,\n",
    "        FC_DROPOUT_PROBABILITIES,\n",
    "        FEATURE_NAMES, \n",
    "        OBJECTIVE_TYPES\n",
    "    )\n",
    ")    \n",
    "\n",
    "_ = [print(t) for t in trial_params]\n",
    "\n",
    "for fold_id, conv_pooling_type, conv_dropout_p, fc_dropout_p, feature_name, objective_type in trial_params:\n",
    "\n",
    "    model_name = f\"ASRCNN\" + \\\n",
    "    f\"__conv_pool_{conv_pooling_type}\" + \\\n",
    "    f\"__conv_dp_{conv_dropout_p}\" + \\\n",
    "    f\"__fc_dp_{fc_dropout_p}\" + \\\n",
    "    f\"__fn_{feature_name}\" + \\\n",
    "    f\"__obj_{objective_type}\"\n",
    "    \n",
    "    model = ASRCNN(\n",
    "        conv_pooling_type, \n",
    "        conv_dropout_p, \n",
    "        fc_dropout_p, \n",
    "        voice_cmd_neuron_count = voice_cmd_class_count, \n",
    "        voice_cmd_lng_neuron_count = voice_cmd_lng_class_count,\n",
    "        objective_type = objective_type\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"{model_name} using {feature_name} on fold#{fold_id}\")\n",
    "\n",
    "    epochs_results = train_on_fold(\n",
    "        model, \n",
    "        fold_id, \n",
    "        feature_name, \n",
    "        objective_type, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        epochs = EPOCHS\n",
    "    )\n",
    "\n",
    "    # results for only one fold\n",
    "    folds_results = [{\n",
    "        'fold_index': fold_id,\n",
    "        'feature_name': feature_name,\n",
    "        'epochs': epochs_results\n",
    "    }]\n",
    "    \n",
    "    \n",
    "    save_results(model_name, folds_results)\n",
    "    # write_epoch_test_logits(model_name, all_folds_results)\n",
    "\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
